{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import  transforms\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset sizes: {'train': 316795, 'val': 56903, 'test': 63966}\n",
      "Filtered classes: ['Play', 'Time_Between']\n",
      "\n",
      "TRAIN dataset:\n",
      "  Play: 128701 frames\n",
      "  Time_Between: 188094 frames\n",
      "\n",
      "VAL dataset:\n",
      "  Play: 23801 frames\n",
      "  Time_Between: 33102 frames\n",
      "\n",
      "TEST dataset:\n",
      "  Play: 26188 frames\n",
      "  Time_Between: 37778 frames\n"
     ]
    }
   ],
   "source": [
    "# Define data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomRotation(degrees=10),\n",
    "        #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  \n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Path to your data directory\n",
    "data_dir = 'Sets'\n",
    "\n",
    "# Load data from directories, focusing on 'Play' and 'Time_Between' classes only\n",
    "def filter_classes(dataset, classes_to_include):\n",
    "    # Filter samples\n",
    "    filtered_samples = [(path, label) for path, label in dataset.samples if dataset.classes[label] in classes_to_include]\n",
    "    \n",
    "    # Reassign targets and samples\n",
    "    new_targets = []\n",
    "    new_samples = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes_to_include)}\n",
    "    \n",
    "    for path, label in filtered_samples:\n",
    "        class_name = dataset.classes[label]\n",
    "        if class_name in class_to_idx:\n",
    "            new_label = class_to_idx[class_name]\n",
    "            new_samples.append((path, new_label))\n",
    "            new_targets.append(new_label)\n",
    "    \n",
    "    dataset.samples = new_samples\n",
    "    dataset.targets = new_targets\n",
    "    dataset.classes = classes_to_include\n",
    "    dataset.class_to_idx = class_to_idx\n",
    "\n",
    "# Apply the filter to each dataset split\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    filter_classes(image_datasets[phase], ['Play', 'Time_Between'])\n",
    "\n",
    "# Check dataset sizes and class names\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Print dataset sizes and class names\n",
    "print(\"Filtered dataset sizes:\", dataset_sizes)\n",
    "print(\"Filtered classes:\", class_names)\n",
    "\n",
    "# Print number of frames for each class in each dataset\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{phase.upper()} dataset:\")\n",
    "    class_counts = {class_name: 0 for class_name in class_names}\n",
    "    for _, label in image_datasets[phase].samples:\n",
    "        class_name = class_names[label]\n",
    "        class_counts[class_name] += 1\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced dataset sizes: {'train': 1666, 'val': 250, 'test': 250}\n",
      "\n",
      "TRAIN dataset:\n",
      "  Play: 49980 frames\n",
      "  Time_Between: 49980 frames\n",
      "\n",
      "VAL dataset:\n",
      "  Play: 7500 frames\n",
      "  Time_Between: 7500 frames\n",
      "\n",
      "TEST dataset:\n",
      "  Play: 7500 frames\n",
      "  Time_Between: 7500 frames\n",
      "\n",
      "TRAIN dataset (after balancing and reducing):\n",
      "  Play: 833 sequences (49980 frames)\n",
      "  Time_Between: 833 sequences (49980 frames)\n",
      "\n",
      "VAL dataset (after balancing and reducing):\n",
      "  Play: 125 sequences (7500 frames)\n",
      "  Time_Between: 125 sequences (7500 frames)\n",
      "\n",
      "TEST dataset (after balancing and reducing):\n",
      "  Play: 125 sequences (7500 frames)\n",
      "  Time_Between: 125 sequences (7500 frames)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to balance the dataset and adjust frame count for each phase\n",
    "def balance_and_reduce_sequences(dataset, seq_length, max_frames_per_class):\n",
    "    # Count the number of samples for each class\n",
    "    class_counts = defaultdict(list)\n",
    "    for idx, (path, label) in enumerate(dataset.samples):\n",
    "        class_name = dataset.classes[label]\n",
    "        class_counts[class_name].append((path, label))\n",
    "\n",
    "    # Determine the maximum number of sequences per class\n",
    "    max_sequences_per_class = max_frames_per_class // seq_length\n",
    "\n",
    "    # Balance the dataset by reducing the larger class sequences\n",
    "    balanced_samples = []\n",
    "    for class_name in class_counts:\n",
    "        samples = class_counts[class_name][:max_sequences_per_class * seq_length]\n",
    "        balanced_samples.extend(samples)\n",
    "    \n",
    "    # Update the dataset with the balanced and reduced samples\n",
    "    dataset.samples = balanced_samples\n",
    "    dataset.targets = [label for _, label in balanced_samples]\n",
    "\n",
    "# Define the sequence length\n",
    "seq_length = 60  # Number of frames per sequence\n",
    "\n",
    "# Define maximum frames for each dataset\n",
    "max_frames_train = 50000\n",
    "max_frames_val_test = 7500\n",
    "\n",
    "# Apply the balance and reduce function to each dataset split\n",
    "balance_and_reduce_sequences(image_datasets['train'], seq_length, max_frames_train)\n",
    "balance_and_reduce_sequences(image_datasets['val'], seq_length, max_frames_val_test)\n",
    "balance_and_reduce_sequences(image_datasets['test'], seq_length, max_frames_val_test)\n",
    "\n",
    "# Continue with creating and using the VideoDataset class, and creating DataLoaders\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, seq_length, transform=None):\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.transform = transform\n",
    "\n",
    "        # Sort the samples to ensure they are in the correct order\n",
    "        self.image_folder_dataset.samples.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder_dataset) // self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = []\n",
    "        labels = []\n",
    "        img_paths = []\n",
    "        for i in range(self.seq_length):\n",
    "            img_path, label = self.image_folder_dataset.samples[idx * self.seq_length + i]\n",
    "            img = self.image_folder_dataset.loader(img_path)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "            img_paths.append(img_path)\n",
    "\n",
    "        images = torch.stack(images)\n",
    "        labels = torch.tensor(labels)\n",
    "        assert torch.all(labels == labels[0])\n",
    "\n",
    "        return images, labels[0], img_paths\n",
    "\n",
    "video_datasets = {x: VideoDataset(image_datasets[x], seq_length, data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(video_datasets[x], batch_size=4, shuffle=True) for x in ['train', 'val', 'test']} #Shuffle wurde von False auf True gesetzt\n",
    "\n",
    "# Check balanced dataset sizes\n",
    "balanced_dataset_sizes = {x: len(video_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "print(\"Balanced dataset sizes:\", balanced_dataset_sizes)\n",
    "\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{phase.upper()} dataset:\")\n",
    "    class_counts = {class_name: 0 for class_name in class_names}\n",
    "    for _, label in image_datasets[phase].samples:\n",
    "        class_name = class_names[label]\n",
    "        class_counts[class_name] += 1\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count} frames\")\n",
    "\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{phase.upper()} dataset (after balancing and reducing):\")\n",
    "    class_counts = {class_name: 0 for class_name in class_names}\n",
    "    for _, label in video_datasets[phase].image_folder_dataset.samples:\n",
    "        class_name = class_names[label]\n",
    "        class_counts[class_name] += 1\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count // seq_length} sequences ({count} frames)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sequence - Label: Play\n",
      "  Sets\\train\\Play\\GH065451_frame_20339.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_2034.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20340.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20341.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20342.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20343.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20344.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20345.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20346.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20347.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20348.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20349.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_2035.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20350.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20351.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20352.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20353.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20354.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20355.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20356.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20357.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20358.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20359.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_2036.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20360.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20361.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20362.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20363.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20364.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20365.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20366.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20367.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20368.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20369.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_2037.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20370.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20371.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20372.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20373.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20374.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20375.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20376.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20377.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20378.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20379.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_2038.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20380.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20381.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20382.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20383.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20384.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20385.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20386.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20387.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20388.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20389.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_2039.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20390.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20391.jpg\n",
      "  Sets\\train\\Play\\GH065451_frame_20392.jpg\n"
     ]
    }
   ],
   "source": [
    "# Select a random sequence from the training dataset and print the frame names\n",
    "def print_random_sequence(dataloader):\n",
    "    dataset = dataloader.dataset\n",
    "    idx = random.randint(0, len(dataset) - 1)\n",
    "    images, label, img_paths = dataset[idx]\n",
    "    print(f\"Random Sequence - Label: {class_names[label]}\")\n",
    "    for img_path in img_paths:\n",
    "        print(f\"  {img_path}\")\n",
    "\n",
    "# Print a random sequence from the training dataset\n",
    "print_random_sequence(dataloaders['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frame Input: Accepts a sequence of images as input, with each image representing a frame\n",
    "\n",
    "CNN Processing:\n",
    "Each frame is processed individually by the CNN (ResNet 50)\n",
    "The CNN acts as a feature extractor, converting each image into a high-dimensional feature vector\n",
    "\n",
    "Pooling:\n",
    "Features from each frame are reduced to a single vector using nn.AdaptiveAvgPool2d((1, 1)), simplifying each frame's information into a fixed-size vector\n",
    "\n",
    "LSTM Processing:\n",
    "The sequence of compacted feature vectors from all frames is fed into the LSTM network\n",
    "LSTM processes the entire sequence, capturing temporal dependencies and patterns across frames\n",
    "\n",
    "Sequence Analysis:\n",
    "The LSTM analyzes the sequence of vectors to understand changes in visual features over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MCI\\anaconda3\\envs\\football_analysis\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MCI\\anaconda3\\envs\\football_analysis\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and use it if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, cnn_model, hidden_size, num_classes=2, num_layers=1):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) # 2048 for ResNet\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, C, H, W = x.size()  # Extract the dimensions of the input\n",
    "\n",
    "        # Reshape input for CNN\n",
    "        \n",
    "        c_in = x.view(batch_size * seq_length, C, H, W) \n",
    "        c_out = self.cnn(c_in)  # Run through CNN for feature extraction\n",
    "        c_out = c_out.view(batch_size, seq_length, -1)  \n",
    "\n",
    "        # Run through LSTM for sequence processing\n",
    "        r_out, (h_n, c_n) = self.lstm(c_out)  # LSTM layer\n",
    "        out = self.fc(r_out[:, -1, :])  # Use last output of the LSTM for classification\n",
    "        return out\n",
    "\n",
    "# Load the pre-trained ResNet-50 model and modify output feature maps\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "cnn_model = nn.Sequential(*list(resnet.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "# Define the hidden size, input size, number of classes, and number of LSTM layers\n",
    "hidden_size = 512\n",
    "num_classes = 2 \n",
    "#num_classes = 1  # Binary classification with single output for BCEWithLogitsLoss\n",
    "num_layers = 1 # Example number of LSTM layers\n",
    "\n",
    "# Instantiate the combined CNN-LSTM model\n",
    "cnn_lstm_model = CNN_LSTM(cnn_model, hidden_size, num_classes, num_layers).to(device)\n",
    "\n",
    "# Define the criterion, optimizer, and learning rate scheduler\n",
    "#criterion = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss for binary classification (Sigmoid)\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification tasks (Softmax)\n",
    "optimizer_ft = optim.Adam(cnn_lstm_model.parameters(), lr=0.0001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start\n",
      "Epoch 0/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.7151 Acc: 0.6050\n",
      "Batch 100/417: train Loss: 0.6750 Acc: 0.6175\n",
      "Batch 150/417: train Loss: 0.6720 Acc: 0.6150\n",
      "Batch 200/417: train Loss: 0.6680 Acc: 0.6100\n",
      "Batch 250/417: train Loss: 0.6687 Acc: 0.6060\n",
      "Batch 300/417: train Loss: 0.6678 Acc: 0.5908\n",
      "Batch 350/417: train Loss: 0.6636 Acc: 0.5814\n",
      "Batch 400/417: train Loss: 0.6598 Acc: 0.5750\n",
      "train Loss: 0.0035 Acc: 0.0030\n",
      "Batch 50/63: val Loss: 0.7049 Acc: 0.5300\n",
      "val Loss: 0.0031 Acc: 0.0022\n",
      "Confusion Matrix for epoch 0:\n",
      "[[77 48]\n",
      " [75 50]]\n",
      "\n",
      "Epoch 1/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.6407 Acc: 0.6000\n",
      "Batch 100/417: train Loss: 0.6448 Acc: 0.6075\n",
      "Batch 150/417: train Loss: 0.6416 Acc: 0.6033\n",
      "Batch 200/417: train Loss: 0.6287 Acc: 0.6038\n",
      "Batch 250/417: train Loss: 0.6273 Acc: 0.6110\n",
      "Batch 300/417: train Loss: 0.6281 Acc: 0.6075\n",
      "Batch 350/417: train Loss: 0.6304 Acc: 0.5964\n",
      "Batch 400/417: train Loss: 0.6301 Acc: 0.5975\n",
      "train Loss: 0.0033 Acc: 0.0031\n",
      "Batch 50/63: val Loss: 0.6888 Acc: 0.4950\n",
      "val Loss: 0.0030 Acc: 0.0022\n",
      "Confusion Matrix for epoch 1:\n",
      "[[ 12 113]\n",
      " [ 11 114]]\n",
      "\n",
      "Epoch 2/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.5897 Acc: 0.6250\n",
      "Batch 100/417: train Loss: 0.5911 Acc: 0.6300\n",
      "Batch 150/417: train Loss: 0.5974 Acc: 0.6167\n",
      "Batch 200/417: train Loss: 0.6064 Acc: 0.6188\n",
      "Batch 250/417: train Loss: 0.6081 Acc: 0.6160\n",
      "Batch 300/417: train Loss: 0.6113 Acc: 0.6150\n",
      "Batch 350/417: train Loss: 0.6134 Acc: 0.6071\n",
      "Batch 400/417: train Loss: 0.6124 Acc: 0.6038\n",
      "train Loss: 0.0032 Acc: 0.0032\n",
      "Batch 50/63: val Loss: 0.6672 Acc: 0.5950\n",
      "val Loss: 0.0029 Acc: 0.0025\n",
      "Confusion Matrix for epoch 2:\n",
      "[[ 41  84]\n",
      " [ 21 104]]\n",
      "\n",
      "Epoch 3/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.6329 Acc: 0.5900\n",
      "Batch 100/417: train Loss: 0.6352 Acc: 0.5975\n",
      "Batch 150/417: train Loss: 0.6274 Acc: 0.6033\n",
      "Batch 200/417: train Loss: 0.6148 Acc: 0.6262\n",
      "Batch 250/417: train Loss: 0.6142 Acc: 0.6260\n",
      "Batch 300/417: train Loss: 0.6058 Acc: 0.6308\n",
      "Batch 350/417: train Loss: 0.6100 Acc: 0.6214\n",
      "Batch 400/417: train Loss: 0.6103 Acc: 0.6244\n",
      "train Loss: 0.0032 Acc: 0.0033\n",
      "Batch 50/63: val Loss: 0.6764 Acc: 0.5700\n",
      "val Loss: 0.0030 Acc: 0.0024\n",
      "Confusion Matrix for epoch 3:\n",
      "[[47 78]\n",
      " [37 88]]\n",
      "\n",
      "Epoch 4/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.5917 Acc: 0.6150\n",
      "Batch 100/417: train Loss: 0.5985 Acc: 0.6200\n",
      "Batch 150/417: train Loss: 0.6179 Acc: 0.6167\n",
      "Batch 200/417: train Loss: 0.6097 Acc: 0.6288\n",
      "Batch 250/417: train Loss: 0.6132 Acc: 0.6300\n",
      "Batch 300/417: train Loss: 0.6019 Acc: 0.6400\n",
      "Batch 350/417: train Loss: 0.6053 Acc: 0.6293\n",
      "Batch 400/417: train Loss: 0.6085 Acc: 0.6200\n",
      "train Loss: 0.0032 Acc: 0.0033\n",
      "Batch 50/63: val Loss: 0.7101 Acc: 0.4900\n",
      "val Loss: 0.0031 Acc: 0.0022\n",
      "Confusion Matrix for epoch 4:\n",
      "[[  0 125]\n",
      " [  0 125]]\n",
      "\n",
      "Epoch 5/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.5905 Acc: 0.6400\n",
      "Batch 100/417: train Loss: 0.5905 Acc: 0.6475\n",
      "Batch 150/417: train Loss: 0.6090 Acc: 0.6400\n",
      "Batch 200/417: train Loss: 0.6055 Acc: 0.6475\n",
      "Batch 250/417: train Loss: 0.6042 Acc: 0.6490\n",
      "Batch 300/417: train Loss: 0.5997 Acc: 0.6500\n",
      "Batch 350/417: train Loss: 0.5996 Acc: 0.6479\n",
      "Batch 400/417: train Loss: 0.5936 Acc: 0.6444\n",
      "train Loss: 0.0031 Acc: 0.0034\n",
      "Batch 50/63: val Loss: 0.6563 Acc: 0.5850\n",
      "val Loss: 0.0029 Acc: 0.0025\n",
      "Confusion Matrix for epoch 5:\n",
      "[[ 42  83]\n",
      " [ 24 101]]\n",
      "\n",
      "Epoch 6/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.5510 Acc: 0.6950\n",
      "Batch 100/417: train Loss: 0.5691 Acc: 0.6975\n",
      "Batch 150/417: train Loss: 0.5543 Acc: 0.7167\n",
      "Batch 200/417: train Loss: 0.5653 Acc: 0.6950\n",
      "Batch 250/417: train Loss: 0.5588 Acc: 0.6970\n",
      "Batch 300/417: train Loss: 0.5652 Acc: 0.6875\n",
      "Batch 350/417: train Loss: 0.5628 Acc: 0.6893\n",
      "Batch 400/417: train Loss: 0.5543 Acc: 0.6925\n",
      "train Loss: 0.0029 Acc: 0.0036\n",
      "Batch 50/63: val Loss: 0.8761 Acc: 0.5500\n",
      "val Loss: 0.0039 Acc: 0.0024\n",
      "Confusion Matrix for epoch 6:\n",
      "[[52 73]\n",
      " [43 82]]\n",
      "\n",
      "Epoch 7/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.5770 Acc: 0.6650\n",
      "Batch 100/417: train Loss: 0.5473 Acc: 0.6950\n",
      "Batch 150/417: train Loss: 0.5387 Acc: 0.7067\n",
      "Batch 200/417: train Loss: 0.5264 Acc: 0.7225\n",
      "Batch 250/417: train Loss: 0.5177 Acc: 0.7280\n",
      "Batch 300/417: train Loss: 0.5106 Acc: 0.7367\n",
      "Batch 350/417: train Loss: 0.5066 Acc: 0.7371\n",
      "Batch 400/417: train Loss: 0.5074 Acc: 0.7344\n",
      "train Loss: 0.0027 Acc: 0.0039\n",
      "Batch 50/63: val Loss: 0.6816 Acc: 0.6050\n",
      "val Loss: 0.0029 Acc: 0.0027\n",
      "Confusion Matrix for epoch 7:\n",
      "[[96 29]\n",
      " [69 56]]\n",
      "\n",
      "Epoch 8/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.4257 Acc: 0.7850\n",
      "Batch 100/417: train Loss: 0.4281 Acc: 0.7825\n",
      "Batch 150/417: train Loss: 0.4247 Acc: 0.7817\n",
      "Batch 200/417: train Loss: 0.4184 Acc: 0.7900\n",
      "Batch 250/417: train Loss: 0.4234 Acc: 0.7850\n",
      "Batch 300/417: train Loss: 0.4281 Acc: 0.7825\n",
      "Batch 350/417: train Loss: 0.4295 Acc: 0.7850\n",
      "Batch 400/417: train Loss: 0.4320 Acc: 0.7825\n",
      "train Loss: 0.0023 Acc: 0.0041\n",
      "Batch 50/63: val Loss: 0.7806 Acc: 0.6150\n",
      "val Loss: 0.0035 Acc: 0.0027\n",
      "Confusion Matrix for epoch 8:\n",
      "[[97 28]\n",
      " [67 58]]\n",
      "\n",
      "Epoch 9/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.4373 Acc: 0.7750\n",
      "Batch 100/417: train Loss: 0.4154 Acc: 0.7875\n",
      "Batch 150/417: train Loss: 0.4026 Acc: 0.7917\n",
      "Batch 200/417: train Loss: 0.4068 Acc: 0.7925\n",
      "Batch 250/417: train Loss: 0.3957 Acc: 0.7970\n",
      "Batch 300/417: train Loss: 0.3932 Acc: 0.8008\n",
      "Batch 350/417: train Loss: 0.3894 Acc: 0.8086\n",
      "Batch 400/417: train Loss: 0.3881 Acc: 0.8113\n",
      "train Loss: 0.0020 Acc: 0.0043\n",
      "Batch 50/63: val Loss: 1.0454 Acc: 0.6300\n",
      "val Loss: 0.0045 Acc: 0.0028\n",
      "Confusion Matrix for epoch 9:\n",
      "[[98 27]\n",
      " [65 60]]\n",
      "\n",
      "Epoch 10/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.3518 Acc: 0.8100\n",
      "Batch 100/417: train Loss: 0.3440 Acc: 0.8400\n",
      "Batch 150/417: train Loss: 0.3639 Acc: 0.8300\n",
      "Batch 200/417: train Loss: 0.3689 Acc: 0.8313\n",
      "Batch 250/417: train Loss: 0.3629 Acc: 0.8330\n",
      "Batch 300/417: train Loss: 0.3695 Acc: 0.8292\n",
      "Batch 350/417: train Loss: 0.3640 Acc: 0.8293\n",
      "Batch 400/417: train Loss: 0.3547 Acc: 0.8350\n",
      "train Loss: 0.0019 Acc: 0.0044\n",
      "Batch 50/63: val Loss: 1.0365 Acc: 0.5950\n",
      "val Loss: 0.0046 Acc: 0.0026\n",
      "Confusion Matrix for epoch 10:\n",
      "[[107  18]\n",
      " [ 82  43]]\n",
      "\n",
      "Epoch 11/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.2628 Acc: 0.8700\n",
      "Batch 100/417: train Loss: 0.2862 Acc: 0.8625\n",
      "Batch 150/417: train Loss: 0.2828 Acc: 0.8717\n",
      "Batch 200/417: train Loss: 0.2815 Acc: 0.8762\n",
      "Batch 250/417: train Loss: 0.2903 Acc: 0.8730\n",
      "Batch 300/417: train Loss: 0.2861 Acc: 0.8750\n",
      "Batch 350/417: train Loss: 0.2926 Acc: 0.8721\n",
      "Batch 400/417: train Loss: 0.2888 Acc: 0.8712\n",
      "train Loss: 0.0015 Acc: 0.0046\n",
      "Batch 50/63: val Loss: 1.5660 Acc: 0.6100\n",
      "val Loss: 0.0067 Acc: 0.0027\n",
      "Confusion Matrix for epoch 11:\n",
      "[[100  25]\n",
      " [ 72  53]]\n",
      "\n",
      "Epoch 12/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.2715 Acc: 0.9000\n",
      "Batch 100/417: train Loss: 0.2511 Acc: 0.9050\n",
      "Batch 150/417: train Loss: 0.2656 Acc: 0.8817\n",
      "Batch 200/417: train Loss: 0.2676 Acc: 0.8875\n",
      "Batch 250/417: train Loss: 0.2675 Acc: 0.8860\n",
      "Batch 300/417: train Loss: 0.2624 Acc: 0.8883\n",
      "Batch 350/417: train Loss: 0.2604 Acc: 0.8886\n",
      "Batch 400/417: train Loss: 0.2557 Acc: 0.8906\n",
      "train Loss: 0.0013 Acc: 0.0047\n",
      "Batch 50/63: val Loss: 1.2152 Acc: 0.6400\n",
      "val Loss: 0.0056 Acc: 0.0028\n",
      "Confusion Matrix for epoch 12:\n",
      "[[96 29]\n",
      " [62 63]]\n",
      "\n",
      "Epoch 13/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1832 Acc: 0.9250\n",
      "Batch 100/417: train Loss: 0.1713 Acc: 0.9325\n",
      "Batch 150/417: train Loss: 0.1865 Acc: 0.9250\n",
      "Batch 200/417: train Loss: 0.1910 Acc: 0.9200\n",
      "Batch 250/417: train Loss: 0.1863 Acc: 0.9230\n",
      "Batch 300/417: train Loss: 0.1886 Acc: 0.9217\n",
      "Batch 350/417: train Loss: 0.1911 Acc: 0.9186\n",
      "Batch 400/417: train Loss: 0.1951 Acc: 0.9169\n",
      "train Loss: 0.0010 Acc: 0.0048\n",
      "Batch 50/63: val Loss: 1.8940 Acc: 0.5850\n",
      "val Loss: 0.0084 Acc: 0.0026\n",
      "Confusion Matrix for epoch 13:\n",
      "[[105  20]\n",
      " [ 84  41]]\n",
      "\n",
      "Epoch 14/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1539 Acc: 0.9400\n",
      "Batch 100/417: train Loss: 0.1687 Acc: 0.9375\n",
      "Batch 150/417: train Loss: 0.1720 Acc: 0.9350\n",
      "Batch 200/417: train Loss: 0.1559 Acc: 0.9425\n",
      "Batch 250/417: train Loss: 0.1584 Acc: 0.9400\n",
      "Batch 300/417: train Loss: 0.1545 Acc: 0.9417\n",
      "Batch 350/417: train Loss: 0.1577 Acc: 0.9386\n",
      "Batch 400/417: train Loss: 0.1571 Acc: 0.9413\n",
      "train Loss: 0.0008 Acc: 0.0049\n",
      "Batch 50/63: val Loss: 1.7222 Acc: 0.6100\n",
      "val Loss: 0.0077 Acc: 0.0027\n",
      "Confusion Matrix for epoch 14:\n",
      "[[100  25]\n",
      " [ 71  54]]\n",
      "\n",
      "Epoch 15/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1731 Acc: 0.9300\n",
      "Batch 100/417: train Loss: 0.1522 Acc: 0.9400\n",
      "Batch 150/417: train Loss: 0.1445 Acc: 0.9450\n",
      "Batch 200/417: train Loss: 0.1389 Acc: 0.9450\n",
      "Batch 250/417: train Loss: 0.1418 Acc: 0.9420\n",
      "Batch 300/417: train Loss: 0.1409 Acc: 0.9458\n",
      "Batch 350/417: train Loss: 0.1464 Acc: 0.9429\n",
      "Batch 400/417: train Loss: 0.1449 Acc: 0.9444\n",
      "train Loss: 0.0008 Acc: 0.0050\n",
      "Batch 50/63: val Loss: 2.1370 Acc: 0.5850\n",
      "val Loss: 0.0093 Acc: 0.0027\n",
      "Confusion Matrix for epoch 15:\n",
      "[[103  22]\n",
      " [ 77  48]]\n",
      "\n",
      "Epoch 16/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1202 Acc: 0.9600\n",
      "Batch 100/417: train Loss: 0.1470 Acc: 0.9450\n",
      "Batch 150/417: train Loss: 0.1307 Acc: 0.9483\n",
      "Batch 200/417: train Loss: 0.1258 Acc: 0.9500\n",
      "Batch 250/417: train Loss: 0.1329 Acc: 0.9500\n",
      "Batch 300/417: train Loss: 0.1331 Acc: 0.9517\n",
      "Batch 350/417: train Loss: 0.1303 Acc: 0.9529\n",
      "Batch 400/417: train Loss: 0.1344 Acc: 0.9519\n",
      "train Loss: 0.0007 Acc: 0.0050\n",
      "Batch 50/63: val Loss: 2.0145 Acc: 0.6350\n",
      "val Loss: 0.0093 Acc: 0.0027\n",
      "Confusion Matrix for epoch 16:\n",
      "[[105  20]\n",
      " [ 79  46]]\n",
      "\n",
      "Epoch 17/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1574 Acc: 0.9400\n",
      "Batch 100/417: train Loss: 0.1492 Acc: 0.9375\n",
      "Batch 150/417: train Loss: 0.1431 Acc: 0.9400\n",
      "Batch 200/417: train Loss: 0.1445 Acc: 0.9413\n",
      "Batch 250/417: train Loss: 0.1388 Acc: 0.9470\n",
      "Batch 300/417: train Loss: 0.1330 Acc: 0.9508\n",
      "Batch 350/417: train Loss: 0.1379 Acc: 0.9479\n",
      "Batch 400/417: train Loss: 0.1347 Acc: 0.9487\n",
      "train Loss: 0.0007 Acc: 0.0050\n",
      "Batch 50/63: val Loss: 2.0144 Acc: 0.6050\n",
      "val Loss: 0.0093 Acc: 0.0026\n",
      "Confusion Matrix for epoch 17:\n",
      "[[104  21]\n",
      " [ 80  45]]\n",
      "\n",
      "Epoch 18/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.0869 Acc: 0.9700\n",
      "Batch 100/417: train Loss: 0.1188 Acc: 0.9600\n",
      "Batch 150/417: train Loss: 0.1342 Acc: 0.9483\n",
      "Batch 200/417: train Loss: 0.1193 Acc: 0.9575\n",
      "Batch 250/417: train Loss: 0.1245 Acc: 0.9520\n",
      "Batch 300/417: train Loss: 0.1259 Acc: 0.9508\n",
      "Batch 350/417: train Loss: 0.1227 Acc: 0.9536\n",
      "Batch 400/417: train Loss: 0.1247 Acc: 0.9537\n",
      "train Loss: 0.0007 Acc: 0.0050\n",
      "Batch 50/63: val Loss: 1.6047 Acc: 0.5950\n",
      "val Loss: 0.0070 Acc: 0.0026\n",
      "Confusion Matrix for epoch 18:\n",
      "[[91 34]\n",
      " [66 59]]\n",
      "\n",
      "Epoch 19/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1325 Acc: 0.9500\n",
      "Batch 100/417: train Loss: 0.1036 Acc: 0.9650\n",
      "Batch 150/417: train Loss: 0.1048 Acc: 0.9650\n",
      "Batch 200/417: train Loss: 0.1049 Acc: 0.9663\n",
      "Batch 250/417: train Loss: 0.1135 Acc: 0.9610\n",
      "Batch 300/417: train Loss: 0.1184 Acc: 0.9583\n",
      "Batch 350/417: train Loss: 0.1160 Acc: 0.9586\n",
      "Batch 400/417: train Loss: 0.1105 Acc: 0.9600\n",
      "train Loss: 0.0006 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 1.9943 Acc: 0.6200\n",
      "val Loss: 0.0094 Acc: 0.0026\n",
      "Confusion Matrix for epoch 19:\n",
      "[[103  22]\n",
      " [ 78  47]]\n",
      "\n",
      "Epoch 20/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1368 Acc: 0.9600\n",
      "Batch 100/417: train Loss: 0.1343 Acc: 0.9525\n",
      "Batch 150/417: train Loss: 0.1198 Acc: 0.9533\n",
      "Batch 200/417: train Loss: 0.1123 Acc: 0.9625\n",
      "Batch 250/417: train Loss: 0.1069 Acc: 0.9660\n",
      "Batch 300/417: train Loss: 0.1078 Acc: 0.9642\n",
      "Batch 350/417: train Loss: 0.1095 Acc: 0.9621\n",
      "Batch 400/417: train Loss: 0.1138 Acc: 0.9594\n",
      "train Loss: 0.0006 Acc: 0.0050\n",
      "Batch 50/63: val Loss: 1.9449 Acc: 0.6150\n",
      "val Loss: 0.0087 Acc: 0.0026\n",
      "Confusion Matrix for epoch 20:\n",
      "[[104  21]\n",
      " [ 79  46]]\n",
      "\n",
      "Epoch 21/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.0900 Acc: 0.9600\n",
      "Batch 100/417: train Loss: 0.0894 Acc: 0.9675\n",
      "Batch 150/417: train Loss: 0.1121 Acc: 0.9600\n",
      "Batch 200/417: train Loss: 0.1190 Acc: 0.9587\n",
      "Batch 250/417: train Loss: 0.1218 Acc: 0.9590\n",
      "Batch 300/417: train Loss: 0.1214 Acc: 0.9608\n",
      "Batch 350/417: train Loss: 0.1194 Acc: 0.9621\n",
      "Batch 400/417: train Loss: 0.1166 Acc: 0.9631\n",
      "train Loss: 0.0006 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.2233 Acc: 0.5950\n",
      "val Loss: 0.0102 Acc: 0.0026\n",
      "Confusion Matrix for epoch 21:\n",
      "[[105  20]\n",
      " [ 81  44]]\n",
      "\n",
      "Epoch 22/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1126 Acc: 0.9600\n",
      "Batch 100/417: train Loss: 0.0906 Acc: 0.9675\n",
      "Batch 150/417: train Loss: 0.0951 Acc: 0.9650\n",
      "Batch 200/417: train Loss: 0.1149 Acc: 0.9600\n",
      "Batch 250/417: train Loss: 0.1206 Acc: 0.9590\n",
      "Batch 300/417: train Loss: 0.1143 Acc: 0.9625\n",
      "Batch 350/417: train Loss: 0.1134 Acc: 0.9607\n",
      "Batch 400/417: train Loss: 0.1088 Acc: 0.9631\n",
      "train Loss: 0.0006 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 1.7847 Acc: 0.5900\n",
      "val Loss: 0.0076 Acc: 0.0027\n",
      "Confusion Matrix for epoch 22:\n",
      "[[104  21]\n",
      " [ 78  47]]\n",
      "\n",
      "Epoch 23/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1258 Acc: 0.9550\n",
      "Batch 100/417: train Loss: 0.1117 Acc: 0.9675\n",
      "Batch 150/417: train Loss: 0.0998 Acc: 0.9700\n",
      "Batch 200/417: train Loss: 0.1075 Acc: 0.9637\n",
      "Batch 250/417: train Loss: 0.1086 Acc: 0.9640\n",
      "Batch 300/417: train Loss: 0.1066 Acc: 0.9617\n",
      "Batch 350/417: train Loss: 0.1129 Acc: 0.9586\n",
      "Batch 400/417: train Loss: 0.1095 Acc: 0.9606\n",
      "train Loss: 0.0006 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 1.9820 Acc: 0.6350\n",
      "val Loss: 0.0093 Acc: 0.0027\n",
      "Confusion Matrix for epoch 23:\n",
      "[[103  22]\n",
      " [ 77  48]]\n",
      "\n",
      "Epoch 24/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1349 Acc: 0.9350\n",
      "Batch 100/417: train Loss: 0.1176 Acc: 0.9450\n",
      "Batch 150/417: train Loss: 0.1110 Acc: 0.9533\n",
      "Batch 200/417: train Loss: 0.1084 Acc: 0.9587\n",
      "Batch 250/417: train Loss: 0.1062 Acc: 0.9600\n",
      "Batch 300/417: train Loss: 0.1086 Acc: 0.9592\n",
      "Batch 350/417: train Loss: 0.1066 Acc: 0.9614\n",
      "Batch 400/417: train Loss: 0.1049 Acc: 0.9625\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 1.8176 Acc: 0.5950\n",
      "val Loss: 0.0078 Acc: 0.0027\n",
      "Confusion Matrix for epoch 24:\n",
      "[[103  22]\n",
      " [ 77  48]]\n",
      "\n",
      "Epoch 25/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.0858 Acc: 0.9700\n",
      "Batch 100/417: train Loss: 0.0846 Acc: 0.9700\n",
      "Batch 150/417: train Loss: 0.0879 Acc: 0.9683\n",
      "Batch 200/417: train Loss: 0.0912 Acc: 0.9700\n",
      "Batch 250/417: train Loss: 0.0890 Acc: 0.9750\n",
      "Batch 300/417: train Loss: 0.0929 Acc: 0.9758\n",
      "Batch 350/417: train Loss: 0.0900 Acc: 0.9764\n",
      "Batch 400/417: train Loss: 0.0979 Acc: 0.9731\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.1036 Acc: 0.6250\n",
      "val Loss: 0.0099 Acc: 0.0026\n",
      "Confusion Matrix for epoch 25:\n",
      "[[106  19]\n",
      " [ 83  42]]\n",
      "\n",
      "Epoch 26/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.0813 Acc: 0.9700\n",
      "Batch 100/417: train Loss: 0.1101 Acc: 0.9550\n",
      "Batch 150/417: train Loss: 0.1026 Acc: 0.9583\n",
      "Batch 200/417: train Loss: 0.1009 Acc: 0.9637\n",
      "Batch 250/417: train Loss: 0.1025 Acc: 0.9640\n",
      "Batch 300/417: train Loss: 0.0998 Acc: 0.9650\n",
      "Batch 350/417: train Loss: 0.1007 Acc: 0.9650\n",
      "Batch 400/417: train Loss: 0.1016 Acc: 0.9650\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.1678 Acc: 0.6200\n",
      "val Loss: 0.0094 Acc: 0.0027\n",
      "Confusion Matrix for epoch 26:\n",
      "[[103  22]\n",
      " [ 77  48]]\n",
      "\n",
      "Epoch 27/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1213 Acc: 0.9500\n",
      "Batch 100/417: train Loss: 0.1069 Acc: 0.9600\n",
      "Batch 150/417: train Loss: 0.0974 Acc: 0.9650\n",
      "Batch 200/417: train Loss: 0.0963 Acc: 0.9675\n",
      "Batch 250/417: train Loss: 0.0918 Acc: 0.9700\n",
      "Batch 300/417: train Loss: 0.0920 Acc: 0.9683\n",
      "Batch 350/417: train Loss: 0.0904 Acc: 0.9693\n",
      "Batch 400/417: train Loss: 0.0951 Acc: 0.9675\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.0777 Acc: 0.6150\n",
      "val Loss: 0.0098 Acc: 0.0026\n",
      "Confusion Matrix for epoch 27:\n",
      "[[106  19]\n",
      " [ 81  44]]\n",
      "\n",
      "Epoch 28/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.0980 Acc: 0.9750\n",
      "Batch 100/417: train Loss: 0.1037 Acc: 0.9725\n",
      "Batch 150/417: train Loss: 0.0908 Acc: 0.9733\n",
      "Batch 200/417: train Loss: 0.0961 Acc: 0.9700\n",
      "Batch 250/417: train Loss: 0.1046 Acc: 0.9670\n",
      "Batch 300/417: train Loss: 0.1078 Acc: 0.9675\n",
      "Batch 350/417: train Loss: 0.1064 Acc: 0.9679\n",
      "Batch 400/417: train Loss: 0.1106 Acc: 0.9650\n",
      "train Loss: 0.0006 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.1363 Acc: 0.5950\n",
      "val Loss: 0.0094 Acc: 0.0027\n",
      "Confusion Matrix for epoch 28:\n",
      "[[103  22]\n",
      " [ 76  49]]\n",
      "\n",
      "Epoch 29/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.0968 Acc: 0.9750\n",
      "Batch 100/417: train Loss: 0.1080 Acc: 0.9675\n",
      "Batch 150/417: train Loss: 0.0972 Acc: 0.9667\n",
      "Batch 200/417: train Loss: 0.0939 Acc: 0.9713\n",
      "Batch 250/417: train Loss: 0.0939 Acc: 0.9720\n",
      "Batch 300/417: train Loss: 0.0969 Acc: 0.9683\n",
      "Batch 350/417: train Loss: 0.0972 Acc: 0.9664\n",
      "Batch 400/417: train Loss: 0.0993 Acc: 0.9656\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.1312 Acc: 0.6150\n",
      "val Loss: 0.0096 Acc: 0.0026\n",
      "Confusion Matrix for epoch 29:\n",
      "[[104  21]\n",
      " [ 79  46]]\n",
      "\n",
      "Epoch 30/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.0858 Acc: 0.9600\n",
      "Batch 100/417: train Loss: 0.1129 Acc: 0.9525\n",
      "Batch 150/417: train Loss: 0.1163 Acc: 0.9550\n",
      "Batch 200/417: train Loss: 0.1070 Acc: 0.9587\n",
      "Batch 250/417: train Loss: 0.1084 Acc: 0.9610\n",
      "Batch 300/417: train Loss: 0.1066 Acc: 0.9625\n",
      "Batch 350/417: train Loss: 0.1029 Acc: 0.9650\n",
      "Batch 400/417: train Loss: 0.1007 Acc: 0.9637\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 1.8045 Acc: 0.5900\n",
      "val Loss: 0.0078 Acc: 0.0026\n",
      "Confusion Matrix for epoch 30:\n",
      "[[97 28]\n",
      " [73 52]]\n",
      "\n",
      "Epoch 31/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1104 Acc: 0.9750\n",
      "Batch 100/417: train Loss: 0.1131 Acc: 0.9675\n",
      "Batch 150/417: train Loss: 0.1116 Acc: 0.9700\n",
      "Batch 200/417: train Loss: 0.1054 Acc: 0.9725\n",
      "Batch 250/417: train Loss: 0.1065 Acc: 0.9700\n",
      "Batch 300/417: train Loss: 0.1013 Acc: 0.9717\n",
      "Batch 350/417: train Loss: 0.1008 Acc: 0.9721\n",
      "Batch 400/417: train Loss: 0.0995 Acc: 0.9719\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.1714 Acc: 0.5900\n",
      "val Loss: 0.0093 Acc: 0.0026\n",
      "Confusion Matrix for epoch 31:\n",
      "[[99 26]\n",
      " [74 51]]\n",
      "\n",
      "Epoch 32/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.0798 Acc: 0.9700\n",
      "Batch 100/417: train Loss: 0.0857 Acc: 0.9675\n",
      "Batch 150/417: train Loss: 0.0856 Acc: 0.9683\n",
      "Batch 200/417: train Loss: 0.0871 Acc: 0.9713\n",
      "Batch 250/417: train Loss: 0.0923 Acc: 0.9730\n",
      "Batch 300/417: train Loss: 0.0883 Acc: 0.9733\n",
      "Batch 350/417: train Loss: 0.0900 Acc: 0.9714\n",
      "Batch 400/417: train Loss: 0.0895 Acc: 0.9725\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.0091 Acc: 0.6150\n",
      "val Loss: 0.0093 Acc: 0.0026\n",
      "Confusion Matrix for epoch 32:\n",
      "[[103  22]\n",
      " [ 78  47]]\n",
      "\n",
      "Epoch 33/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.0869 Acc: 0.9750\n",
      "Batch 100/417: train Loss: 0.0713 Acc: 0.9825\n",
      "Batch 150/417: train Loss: 0.0778 Acc: 0.9767\n",
      "Batch 200/417: train Loss: 0.0837 Acc: 0.9725\n",
      "Batch 250/417: train Loss: 0.0852 Acc: 0.9720\n",
      "Batch 300/417: train Loss: 0.0921 Acc: 0.9683\n",
      "Batch 350/417: train Loss: 0.0925 Acc: 0.9679\n",
      "Batch 400/417: train Loss: 0.0945 Acc: 0.9669\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.1634 Acc: 0.6050\n",
      "val Loss: 0.0095 Acc: 0.0027\n",
      "Confusion Matrix for epoch 33:\n",
      "[[102  23]\n",
      " [ 76  49]]\n",
      "\n",
      "Epoch 34/34\n",
      "----------\n",
      "Batch 50/417: train Loss: 0.1094 Acc: 0.9650\n",
      "Batch 100/417: train Loss: 0.1130 Acc: 0.9625\n",
      "Batch 150/417: train Loss: 0.1048 Acc: 0.9683\n",
      "Batch 200/417: train Loss: 0.1058 Acc: 0.9675\n",
      "Batch 250/417: train Loss: 0.1005 Acc: 0.9670\n",
      "Batch 300/417: train Loss: 0.1025 Acc: 0.9675\n",
      "Batch 350/417: train Loss: 0.1011 Acc: 0.9664\n",
      "Batch 400/417: train Loss: 0.1045 Acc: 0.9656\n",
      "train Loss: 0.0005 Acc: 0.0051\n",
      "Batch 50/63: val Loss: 2.1874 Acc: 0.6000\n",
      "val Loss: 0.0096 Acc: 0.0026\n",
      "Confusion Matrix for epoch 34:\n",
      "[[103  22]\n",
      " [ 78  47]]\n",
      "\n",
      "Training complete in 4715m 36s\n",
      "Best val Acc: 0.0028\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=35, batch_update_interval=50):\n",
    "    since = time.time()  # Track the start time for training duration\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  # Keep a copy of the best model weights\n",
    "    best_acc = 0.0  # Initialize the best accuracy\n",
    "\n",
    "    print(\"Training start\")  # Print training start message\n",
    "\n",
    "    for epoch in range(num_epochs):  # Loop over epochs\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:  # Each epoch has a training and validation phase\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0  # Initialize running loss\n",
    "            running_corrects = 0  # Initialize running correct predictions\n",
    "            batch_count = 0  # Initialize batch count\n",
    "\n",
    "            all_labels = []  # Store all true labels\n",
    "            all_preds = []  # Store all predictions\n",
    "\n",
    "            data_iter = iter(dataloaders[phase])  # Create an iterator for the DataLoader\n",
    "            batch_total = len(dataloaders[phase])  # Total number of batches\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = next(data_iter)  # Get the next batch\n",
    "                    inputs, labels, _ = batch  # Get the first two elements only, ignore the rest\n",
    "                except StopIteration:\n",
    "                    break  # Exit the loop if there are no more batches\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).long()  # Ensure labels are of type long for CrossEntropyLoss\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Update running loss and correct predictions\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                batch_count += 1\n",
    "\n",
    "                # Collect labels and predictions for confusion matrix\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                # Print update every `batch_update_interval` batches\n",
    "                if batch_count % batch_update_interval == 0:\n",
    "                    print(f'Batch {batch_count}/{batch_total}: {phase} Loss: {running_loss / (batch_count * inputs.size(0)):.4f} Acc: {running_corrects.double() / (batch_count * inputs.size(0)):.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "            # Calculate epoch loss and accuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Print confusion matrix for validation phase\n",
    "            if phase == 'val':\n",
    "                cm = confusion_matrix(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n",
    "                print(f'Confusion Matrix for epoch {epoch}:\\n{cm}')\n",
    "\n",
    "            # Deep copy the best model weights and save the model if it has the best accuracy\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, 'best_model.pth')  # Save the best model weights\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Calculate total training time\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Instantiate the criterion for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate the model with live updates and confusion matrix printing\n",
    "model_ft = train_model(cnn_lstm_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=35)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Derzeit: Model ohne Sigmoid weil loss bereits einen sigmoid verwendet im Training.\n",
    "- Versucb mit evtl. batch_size 1 \n",
    "- class number auf 2 => neue Loss function\n",
    "\n",
    "Nochmal die label und Formatierung checken\n",
    "\n",
    "Stand 22.05\n",
    "Neue Loss function, num_classes = 2 \n",
    "[[ 72 594]\n",
    " [ 96 570]]\n",
    " Nach 25 epoch => enrneuter Durchlauf mit 50 \n",
    "\n",
    "Test Nummer 2: \n",
    "Gleicher Code, lr 0.0001, 25 epochs \n",
    "[[300 366]\n",
    " [294 372]]\n",
    "\n",
    " Problem bei diesen Tests: Alle Sets also train und val waren gleichgroß\n",
    "\n",
    "23.05.\n",
    "Erneuter Test mit gleichem Code, aber Erhöhung der epochs auf 50 und 70/15/15 Verhältnis der Sets\n",
    "[[  0 250]\n",
    " [  0 250]]\n",
    "\n",
    "\n",
    "24.05.\n",
    "Test mit neuem Code und neuen Modell BC_2\n",
    "Fehler bei Trainingsschleife, nur eine Klasse beachtet\n",
    "\n",
    "Bei erneutem Test mit BC2 selbes Problem wie bei allen Sigmoid Anwendungen \n",
    "=> Detektion von nur einer Klasse\n",
    "\n",
    "TO DO:\n",
    "Val Datensatz prüfen\n",
    "\n",
    "Test 24.05. Epochs 35\n",
    "[[112 138]\n",
    " [115 135]]\n",
    "\n",
    "Test 25.05. Epochs 35\n",
    "Neue Sequenzgröße von 60 => Verdoppelung \n",
    "[[62 63]\n",
    " [63 62]]\n",
    "\n",
    "\n",
    "Test 26.05.\n",
    "Ohne Data Augmentation, Sequenz 60 \n",
    "[[83 42]\n",
    " [70 55]]\n",
    "\n",
    "Test 27.05.\n",
    "Ohne DA, Sequenz 90\n",
    "[[24 59]\n",
    " [31 52]]\n",
    "\n",
    "Test 28.05\n",
    "Neuer Code, welcher gleiches Modell verwendet. Aber getrennte Sequenzen mit 640 Frames betrachtet. Durch padding werden kleinere Sequnezen aufgefüllt. \n",
    "Abbruch nach 33 Stunden und 18 epochs, da loss und acc gleichbleibend war:\n",
    "val Loss: 0.6935 Acc: 0.5000\n",
    "Confusion Matrix for epoch 17:\n",
    "[[15  0]\n",
    " [15  0]]\n",
    "\n",
    "\n",
    "STAND 29.05\n",
    "Binary_Classification Code ist bester. Nach Meeting wird bestehender Ansatz weiter verbessert\n",
    "=> neues Skript aufbauend auf Binary_Classification \n",
    "Zusätzlich wurde bis jetzt eine batch_size von 1 verwendet, welche evtl. negative Auswirkungen hat, daher werden weitere Versuche mit höherer Batch_size durchgefürt (shuffle = True)\n",
    "\n",
    "Test 29.05\n",
    "Erneute ausführung des Binary_classification Codes\n",
    "- Sequence = 60\n",
    "- shuffle = True und Batch size 4\n",
    "- Layer = 512\n",
    "- Scheduler = 7\n",
    "\n",
    "[[103  22]\n",
    " [ 77  48]]\n",
    " Bislang bestes Ergebnis mit 62 %\n",
    "\n",
    "Überlegung layer size anpassen, für Test kleiner als 512, zB 256, 128\n",
    "Learning Rate Scheduler auf jede bzw. jede 2. epoche anpassen\n",
    "Eventuelle von AvgPooling auf MaxPooling wechseln \n",
    "\n",
    "Test 02.06\n",
    "Binary_SequenceRow_Classification \n",
    "- Sequence = 60\n",
    "- Batch_size = 2\n",
    "- Layer = 256\n",
    "- Scheduler = 2\n",
    "\n",
    "Bestes Ergebnis: ~ 71.5%\n",
    "[[2096  875]\n",
    " [ 814 2156]]\n",
    "\n",
    "Hat sehr gut funktioniert, jedoch war DataLoader nicht optimal. Es waren teilweise beide Klassen in einer Sequenz. Diese Sequenzen wurden verworfen.\n",
    "Ein weiterer Testlauf mit gleichem Code wäre sinnvoll (insofern Test vom 06.06 keine besseren Ergebnisse bringt). Bei neuem Test wird DataLoader angepasst.\n",
    "\n",
    "Test 06.06 \n",
    "Binary_Classification_SR_Seperation \n",
    "Sequenzen enthalten beide Label, es kommt daher innerhalb einer Sequenz zum wechsel einer Klasse\n",
    "- Sequence = 60\n",
    "- Batch_size = 8\n",
    "- Layer = 256\n",
    "- Scheduler = 2\n",
    "- MaxPool2d statt AvgPooling\n",
    "\n",
    "Bestes Ergebnis: 55% nach knapp 2 Tagen Laufzeit \n",
    "[[14378  9052]\n",
    " [10694  8956]]\n",
    "\n",
    "\n",
    "Problem bei diesen Ansatz: Um Labelwechsel zu erkennen, wird für jedes einzelne frame eine Erkennung durchgeführt. Also für jedes frame innerhalb der Sequenz\n",
    "=> kann zu niedriger Genauigkeit führen weil nicht ganze Sequenz analysiert wird \n",
    "=> NOTWENDIGE ANPASSUNG: Das weiterhin nur das letzte Frame der beachtet wird und durch um eins weiterschieben der Label wechsel erkannt wird \n",
    "\n",
    "Nächster Test: Binary_SequenceRow_Classification mit angepasstem DataLoader \n",
    "\n",
    "TEST 10.06.24\n",
    "Binary_SequenceRow_Classification mit angepasstem DataLoader: \n",
    "Sequenzen können nur aus einem Video gebildet werden \n",
    "Sequenzen kontrollieren gesamten Pfad, um Sequenz zu erstellen \n",
    "Überlappung innerhalb der Sequenz weitherin vorhanden \n",
    "\n",
    "- Sequence = 60\n",
    "- Batch_size = 2\n",
    "- Layer = 256\n",
    "- Scheduler = 4\n",
    "\n",
    "\n",
    "TEST 11.06.24\n",
    "Binary_SequenceRow_Classification \n",
    "- Sequence = 60\n",
    "- Batch_size = 2\n",
    "- Layer = 126\n",
    "- num_layer = 2\n",
    "- Scheduler = 2\n",
    "- nn.AdaptiveAvgPool2d\n",
    "\n",
    "Batch 1450/1471: val Loss: 0.5096 Acc: 0.8559\n",
    "[[1325  146]\n",
    " [ 278 1192]]\n",
    "\n",
    "TEST 12.06.24\n",
    "- Sequence = 60\n",
    "- Batch_size = 2\n",
    "- Layer = 128\n",
    "- num_layer = 2\n",
    "- Scheduler = 2\n",
    "- nn.AdaptiveAvgPool2d\n",
    "\n",
    "Neuer DataLoader: Sequenzen können nur aus einem Video gebildet werden\n",
    "Überlappung innerhalb der Sequenz weitherin vorhanden \n",
    "\n",
    "[[1379   92]\n",
    " [ 254 1217]]\n",
    "\n",
    "Genaugkeit knapp 88%\n",
    "\n",
    "Überprüfung mit Test-Datensatz \n",
    "bestmodel 1\n",
    "63%\n",
    "[[1159  312]\n",
    " [ 769  702]]\n",
    "\n",
    "TEST 13.06\n",
    "\n",
    "- Sequence = 60\n",
    "- Batch_size = 2\n",
    "- Layer = 126\n",
    "- num_layer = 1\n",
    "- Scheduler = 2\n",
    "- nn.AdaptiveAvgPool2d\n",
    "val acc: 88%\n",
    "\n",
    "[[648  13]\n",
    " [143 518]]\n",
    "\n",
    " Test:\n",
    " bestmodel 2\n",
    " 61%\n",
    " [[566  95]\n",
    " [431 230]]\n",
    "\n",
    "\n",
    "TEST 14.06\n",
    "seq_length = 60  \n",
    "max_train_sequences = 1000\n",
    "Neuer Dataloader => multi label werden nicht trainiert\n",
    "Stark verringerte Sequenzanzahl \n",
    "\n",
    "Misserfolg, viele Epochs haben beide Klassen als eines erkannt => neuer Versuch \n",
    "\n",
    "TEST 23.06\n",
    "mit gesamten Datensatz \n",
    "[[3640 5857]\n",
    " [2490 7007]]\n",
    "\n",
    "TRAIN dataset: 126630 sequences\n",
    "  Label 0: 63315 sequences\n",
    "  Label 1: 63315 sequences\n",
    "VAL dataset: 18994 sequences\n",
    "  Label 0: 9497 sequences\n",
    "  Label 1: 9497 sequences\n",
    "TEST dataset: 18994 sequences\n",
    "  Label 0: 9497 sequences\n",
    "  Label 1: 9497 sequences\n",
    "\n",
    "\n",
    "Augmentation \n",
    "[[ 662  838]\n",
    " [ 345 1155]] Test \n",
    " \n",
    "\n",
    " [[1002  498]\n",
    " [ 476 1024]]  Val "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
