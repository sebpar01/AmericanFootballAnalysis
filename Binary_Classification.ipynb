{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Classification \n",
    "\n",
    "Author: Sebastian Pareiss \n",
    "Year: 2024 \n",
    "\n",
    "In this Jupyter Notebook, a neural network for binary classification is created. Each cell includes a brief summary of its respective task. This notebook was created with the assistance of ChatGPT 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries and packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import  transforms\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation and Processing\n",
    "\n",
    "- Data Transformations: Define transformations including resizing to 224x224, tensor conversion, and normalization.\n",
    "- Data Directory: Set path to training, validation, and test data.\n",
    "- Class Filtering Function: Filter datasets to include only 'Play' and 'Time_Between' classes.\n",
    "- Dataset Application: Apply filters across training, validation, and test datasets.\n",
    "- Dataset Summary: Print sizes and class distributions within the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomRotation(degrees=10),\n",
    "        #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  \n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Path to data directory\n",
    "data_dir = 'Sets'\n",
    "\n",
    "# Load data from directories, focusing on 'Play' and 'Time_Between' classes only\n",
    "def filter_classes(dataset, classes_to_include):\n",
    "    # Filter samples\n",
    "    filtered_samples = [(path, label) for path, label in dataset.samples if dataset.classes[label] in classes_to_include]\n",
    "    \n",
    "    # Reassign targets and samples\n",
    "    new_targets = []\n",
    "    new_samples = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes_to_include)}\n",
    "    \n",
    "    for path, label in filtered_samples:\n",
    "        class_name = dataset.classes[label]\n",
    "        if class_name in class_to_idx:\n",
    "            new_label = class_to_idx[class_name]\n",
    "            new_samples.append((path, new_label))\n",
    "            new_targets.append(new_label)\n",
    "    \n",
    "    dataset.samples = new_samples\n",
    "    dataset.targets = new_targets\n",
    "    dataset.classes = classes_to_include\n",
    "    dataset.class_to_idx = class_to_idx\n",
    "\n",
    "# Apply the filter to each dataset split\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    filter_classes(image_datasets[phase], ['Play', 'Time_Between'])\n",
    "\n",
    "# Check dataset sizes and class names\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Print dataset sizes and class names\n",
    "print(\"Filtered dataset sizes:\", dataset_sizes)\n",
    "print(\"Filtered classes:\", class_names)\n",
    "\n",
    "# Print number of frames for each class in each dataset\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{phase.upper()} dataset:\")\n",
    "    class_counts = {class_name: 0 for class_name in class_names}\n",
    "    for _, label in image_datasets[phase].samples:\n",
    "        class_name = class_names[label]\n",
    "        class_counts[class_name] += 1\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count} frames\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Dataset Processing:\n",
    "\n",
    "- VideoDataset Class: Custom class to handle video sequences, including sequence creation, transformation application, and data loading.\n",
    "- Initialization: Set up with parameters for the image folder dataset, sequence length, and optional transformations.\n",
    "- Sequence Handling: Generate overlapping sequences for use in training, ensuring proper label consistency across each sequence.\n",
    "- Dataset and DataLoader Creation: Instantiate video datasets for train, validation, and test splits and configure dataloaders with specified batch sizes and shuffling.\n",
    "- Output Information: Print the number of sequences available in each dataset, ensuring clarity on dataset size and composition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, seq_length, transform=None):\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.transform = transform\n",
    "        self.image_folder_dataset.samples.sort()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the number of overlapping sequences\n",
    "        return len(self.image_folder_dataset.samples) - self.seq_length + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_frame = idx\n",
    "        end_frame = start_frame + self.seq_length\n",
    "        images, labels, img_paths = [], [], []\n",
    "\n",
    "        for i in range(start_frame, end_frame):\n",
    "            wrapped_index = i % len(self.image_folder_dataset.samples)\n",
    "            img_path, label = self.image_folder_dataset.samples[wrapped_index]\n",
    "            img = self.image_folder_dataset.loader(img_path)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "            img_paths.append(img_path)\n",
    "\n",
    "        # count label in sequence\n",
    "        label_counts = Counter(labels)\n",
    "        most_common_label, most_common_count = label_counts.most_common(1)[0]\n",
    "\n",
    "    # check label in sequence\n",
    "        for label in labels:\n",
    "            if label != most_common_label:\n",
    "                print(f\"Label mismatch in sequence starting at index {idx}: expected {most_common_label}, found {label}\")\n",
    "\n",
    "        images = torch.stack(images)\n",
    "        return images, torch.tensor(most_common_label), img_paths\n",
    "    \n",
    "# Define sequence length and frame limits\n",
    "seq_length = 60\n",
    "max_frames_train = 10000\n",
    "max_frames_val_test = 1500\n",
    "\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "video_datasets = {x: VideoDataset(image_datasets[x], seq_length, data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(video_datasets[x], batch_size=2, shuffle=True) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# Print dataset sizes\n",
    "#print(\"Balanced dataset sizes (non-overlapping):\", balanced_dataset_sizes)\n",
    "print(\"Dataset sizes with overlapping sequences:\")\n",
    "for key, dataset in video_datasets.items():\n",
    "    print(f\"{key.upper()} dataset: {len(dataset)} sequences\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, seq_length, max_sequences=None, transform=None):\n",
    "      \n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.transform = transform\n",
    "        self.sequences = []\n",
    "        self.label_distribution = defaultdict(int)\n",
    "\n",
    "        # Group frames by directory (label) and video ID\n",
    "        video_label_frames = defaultdict(list)\n",
    "        for path, label in self.image_folder_dataset.samples:\n",
    "            # Split path elements\n",
    "            path_elements = path.split(os.sep)\n",
    "            video_id = path_elements[-1].split('_frame_')[0]\n",
    "            label_dir = path_elements[-2]\n",
    "            full_id = f\"{label_dir}_{video_id}\"\n",
    "            video_label_frames[full_id].append((path, label))\n",
    "\n",
    "        # Process frames for each unique video-label combination\n",
    "        for frames in video_label_frames.values():\n",
    "            # Sort frames within the same video and label by frame number\n",
    "            frames.sort(key=lambda x: int(x[0].split('_frame_')[1].split('.')[0]))\n",
    "\n",
    "            # Create valid sequences\n",
    "            for i in range(len(frames) - seq_length + 1):\n",
    "                sequence = frames[i:i + seq_length]\n",
    "                if len(set(label for _, label in sequence)) == 1:  # Check if all labels in sequence are the same\n",
    "                    self.sequences.append(sequence)\n",
    "                    self.label_distribution[label] += 1\n",
    "\n",
    "        # Balance and limit number of sequences if max_sequences is set\n",
    "        if max_sequences:\n",
    "            balanced_sequences = []\n",
    "            min_count = min(self.label_distribution.values(), default=0)  # Avoid division by zero\n",
    "            limit_per_label = min(max_sequences // 2, min_count)\n",
    "\n",
    "            label_counters = defaultdict(int)\n",
    "            for seq in self.sequences:\n",
    "                label = seq[0][1]\n",
    "                if label_counters[label] < limit_per_label:\n",
    "                    balanced_sequences.append(seq)\n",
    "                    label_counters[label] += 1\n",
    "\n",
    "            self.sequences = balanced_sequences\n",
    "            self.label_distribution = label_counters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.sequences[idx]\n",
    "        images, labels, img_paths = [], [], []\n",
    "        for img_path, label in frames:\n",
    "            img = self.image_folder_dataset.loader(img_path)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "            labels.append(label)\n",
    "            img_paths.append(img_path)\n",
    "        \n",
    "        images = torch.stack(images)\n",
    "        return images, torch.tensor(labels[0]), img_paths\n",
    "\n",
    "# Setup your datasets and dataloaders\n",
    "seq_length = 60  # Desired sequence length\n",
    "max_train_sequences = 20000  # Maximum allowable training sequences\n",
    "\n",
    "# Proportion of test and validation sequences\n",
    "max_val_test_sequences = int(0.15 * max_train_sequences)\n",
    "\n",
    "# Configure individual datasets with respective constraints for train, val, and test\n",
    "video_datasets = {\n",
    "    'train': VideoDataset(image_datasets['train'], seq_length, max_train_sequences, data_transforms['train']),\n",
    "    'val': VideoDataset(image_datasets['val'], seq_length, max_val_test_sequences, data_transforms['val']),\n",
    "    'test': VideoDataset(image_datasets['test'], seq_length, max_val_test_sequences, data_transforms['test'])\n",
    "}\n",
    "\n",
    "# Create dataloaders\n",
    "dataloaders = {\n",
    "    x: DataLoader(video_datasets[x], batch_size=2, shuffle=True) for x in ['train', 'val', 'test']\n",
    "}\n",
    "\n",
    "# Print information about loaded data\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"{phase.upper()} dataset: {len(video_datasets[phase])} sequences\")\n",
    "    for label in range(2):  # Assuming labels are 0 and 1\n",
    "        print(f\"  Label {label}: {video_datasets[phase].label_distribution[label]} sequences\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN-LSTM Model for Binary Classification:\n",
    "\n",
    "- Device Setup: Checks CUDA availability for GPU acceleration and sets the computation device accordingly\n",
    "- CNN-LSTM Architecture: Defines a hybrid neural network model combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) \n",
    "    - CNN: Utilizes a pre-trained ResNet-50 model, adapted for feature extraction\n",
    "    - LSTM: Processes temporal sequences with configurable hidden sizes and layer counts\n",
    "- Model Instantiation: Constructs the CNN-LSTM model with specific parameters and assigns it to the chosen computation device\n",
    "- Training Setup: Establishes the loss function, optimizer, and learning rate scheduler to guide the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and use it if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, cnn_model, hidden_size, num_classes=2, num_layers=1):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) # 2048 for ResNet\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, C, H, W = x.size()  # Extract the dimensions of the input\n",
    "\n",
    "        # Reshape input for CNN\n",
    "        \n",
    "        c_in = x.view(batch_size * seq_length, C, H, W) \n",
    "        c_out = self.cnn(c_in)  # Run through CNN for feature extraction\n",
    "        c_out = c_out.view(batch_size, seq_length, -1)  \n",
    "\n",
    "        # Run through LSTM for sequence processing\n",
    "        r_out, (h_n, c_n) = self.lstm(c_out)  # LSTM layer\n",
    "        out = self.fc(r_out[:, -1, :])  # Use last output of the LSTM for classification\n",
    "        return out\n",
    "\n",
    "# Load the pre-trained ResNet-50 model and modify output feature maps\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "cnn_model = nn.Sequential(*list(resnet.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "# Define the hidden size, input size, number of classes, and number of LSTM layers\n",
    "hidden_size = 128\n",
    "num_classes = 2 \n",
    "num_layers = 1 # Example number of LSTM layers\n",
    "\n",
    "# Instantiate the combined CNN-LSTM model\n",
    "cnn_lstm_model = CNN_LSTM(cnn_model, hidden_size, num_classes, num_layers).to(device)\n",
    "\n",
    "# Define the criterion, optimizer, and learning rate scheduler\n",
    "#criterion = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss for binary classification (Sigmoid)\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification tasks (Softmax)\n",
    "\n",
    "optimizer_ft = optim.Adam(cnn_lstm_model.parameters(), lr=0.0001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluation Process for the CNN-LSTM Model:\n",
    "\n",
    "- Initialization: Sets up the training process, initializing the best model weights and tracking training duration\n",
    "- Training Loop: Iterates over the specified number of epochs, dividing each epoch into training and validation phases\n",
    "- Batch Processing: Processes each batch, applying model predictions, calculating loss, and updating model parameters during the training phase\n",
    "- Performance Metrics: Calculates and displays running loss and accuracy for each batch, updating periodically to monitor performance\n",
    "- Validation and Metrics Storage: In the validation phase, calculates the confusion matrix and updates the best model if improved accuracy is detected\n",
    "- Finalization: Completes training, prints total duration, and loads the best model weights for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=1, batch_update_interval=100):\n",
    "    since = time.time()  # Track the start time for training duration\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  # Keep a copy of the best model weights\n",
    "    best_acc = 0.0  # Initialize the best accuracy\n",
    "\n",
    "    print(\"Training start\")  # Print training start message\n",
    "\n",
    "    for epoch in range(num_epochs):  # Loop over epochs\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:  # Each epoch has a training and validation phase\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0  # Initialize running los9*/\n",
    "        \n",
    "            running_corrects = 0  # Initialize running correct predictions\n",
    "            batch_count = 0  # Initialize batch count\n",
    "\n",
    "            all_labels = []  # Store all true labels\n",
    "            all_preds = []  # Store all predictions\n",
    "\n",
    "            data_iter = iter(dataloaders[phase])  # Create an iterator for the DataLoader\n",
    "            batch_total = len(dataloaders[phase])  # Total number of batches\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = next(data_iter)  # Get the next batch\n",
    "                    inputs, labels, _ = batch  # Get the first two elements only, ignore the rest\n",
    "                except StopIteration:\n",
    "                    break  # Exit the loop if there are no more batches\n",
    "\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).long()  # Ensure labels are of type long for CrossEntropyLoss\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Update running loss and correct predictions\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                batch_count += 1\n",
    "\n",
    "                # Collect labels and predictions for confusion matrix\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                # Print update every `batch_update_interval` batches\n",
    "                if batch_count % batch_update_interval == 0:\n",
    "                    print(f'Batch {batch_count}/{batch_total}: {phase} Loss: {running_loss / (batch_count * inputs.size(0)):.4f} Acc: {running_corrects.double() / (batch_count * inputs.size(0)):.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "            # Calculate epoch loss and accuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Print confusion matrix for validation phase\n",
    "            if phase == 'val':\n",
    "                cm = confusion_matrix(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n",
    "                print(f'Confusion Matrix for epoch {epoch}:\\n{cm}')\n",
    "\n",
    "            # Deep copy the best model weights and save the model if it has the best accuracy\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, 'best_model.pth')  # Save the best model weights\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Calculate total training time\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    #print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Instantiate the criterion for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate the model with live updates and confusion matrix printing\n",
    "model_ft = train_model(cnn_lstm_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the CNN-LSTM Model on Test Data:\n",
    "\n",
    "- Model Loading: Loads the trained CNN-LSTM model from a specified file path and sets it to evaluation mode.\n",
    "- Loss and Accuracy Tracking: Initializes counters for running loss and correct predictions, and prepares to collect all labels and predictions for confusion matrix analysis\n",
    "- Batch Evaluation: Processes each batch from the test DataLoader, performing a forward pass to compute losses and predictions without gradient updates\n",
    "- Metrics Computation: Calculates final accuracy and loss for the test set and constructs the confusion matrix\n",
    "- Results Output: Prints the confusion matrix, final accuracy, and loss, providing a comprehensive evaluation of the model's performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, dataloaders, dataset_sizes, device):\n",
    "    # Load the trained model\n",
    "    model = cnn_lstm_model\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Initialize the confusion matrix calculation\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, _ in dataloaders['test']:  # Adjusted to handle three return values\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass to get outputs\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Update the running loss and corrects counters\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # Compute final accuracy and loss for the entire test set\n",
    "    final_acc = running_corrects.double() / dataset_sizes['test']\n",
    "    final_loss = running_loss / dataset_sizes['test']\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Output the evaluation results\n",
    "    print(f'Confusion Matrix:\\n{cm}')\n",
    "    print(f'Final Accuracy: {final_acc:.4f}')\n",
    "    print(f'Final Loss: {final_loss:.4f}')\n",
    "\n",
    "# Configuration for evaluation\n",
    "model_path = 'best_model_play.pth'  # Path to your model file\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assume dataloaders and dataset_sizes are already defined\n",
    "# dataloaders = {'test': DataLoader(...)}  # Make sure to define this correctly\n",
    "# dataset_sizes = {'test': len(test_dataset)}  # And this\n",
    "\n",
    "# Call the evaluation function\n",
    "evaluate_model(model_path, dataloaders, dataset_sizes, device)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
