{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass Classification (Play Type Classification)\n",
    "\n",
    "Author: Sebastian Pareiss \n",
    "Year: 2024 \n",
    "\n",
    "In this Jupyter Notebook, a neural network for multiclass classification is created. Each cell includes a brief summary of its respective task. This notebook was created with the assistance of ChatGPT 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all libraries and packages\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import  transforms\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation and Processing\n",
    "\n",
    "- Data Transformations: Define transformations including resizing to 224x224, tensor conversion, and normalization\n",
    "- Data Directory: Set path to training, validation, and test data\n",
    "- Class Filtering Function: Filter datasets to include only 'Run', 'Pass', 'Punt', 'Kick-Off' and 'Field Goal' classes\n",
    "- Dataset Application: Apply filters across training, validation, and test datasets\n",
    "- Dataset Summary: Print sizes and class distributions within the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomRotation(degrees=10),\n",
    "        #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  \n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Path to your data directory\n",
    "data_dir = 'Sets'\n",
    "\n",
    "# Load data from directories, focusing on 'Play' and 'Time_Between' classes only\n",
    "def filter_classes(dataset, classes_to_include):\n",
    "    # Filter samples\n",
    "    filtered_samples = [(path, label) for path, label in dataset.samples if dataset.classes[label] in classes_to_include]\n",
    "    \n",
    "    # Reassign targets and samples\n",
    "    new_targets = []\n",
    "    new_samples = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes_to_include)}\n",
    "    \n",
    "    for path, label in filtered_samples:\n",
    "        class_name = dataset.classes[label]\n",
    "        if class_name in class_to_idx:\n",
    "            new_label = class_to_idx[class_name]\n",
    "            new_samples.append((path, new_label))\n",
    "            new_targets.append(new_label)\n",
    "    \n",
    "    dataset.samples = new_samples\n",
    "    dataset.targets = new_targets\n",
    "    dataset.classes = classes_to_include\n",
    "    dataset.class_to_idx = class_to_idx\n",
    "\n",
    "# Apply the filter to each dataset split\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    filter_classes(image_datasets[phase], ['Run', 'Pass', 'Punt', 'Kick-Off', 'Field_Goal'])\n",
    "\n",
    "# Check dataset sizes and class names\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Print dataset sizes and class names\n",
    "print(\"Filtered dataset sizes:\", dataset_sizes)\n",
    "print(\"Filtered classes:\", class_names)\n",
    "\n",
    "# Print number of frames for each class in each dataset\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{phase.upper()} dataset:\")\n",
    "    class_counts = {class_name: 0 for class_name in class_names}\n",
    "    for _, label in image_datasets[phase].samples:\n",
    "        class_name = class_names[label]\n",
    "        class_counts[class_name] += 1\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count} frames\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Dataset Processing:\n",
    "\n",
    "- Custom Dataset Class (VideoDataset): Constructs video datasets by processing directories of frame sequences, ensuring all frames in a sequence share the same label. This class also handles potential sequence imbalance by limiting the number of sequences per label if a maximum is specified\n",
    "- Initialization Parameters: Includes settings for the sequence length and optional transformation application to each frame\n",
    "Sequence Processing: Frames are grouped by video and label, sorted, and checked for consistency before being compiled into sequences. Sequences with mismatched labels within their frames are discarded.\n",
    "- Dataset Setup: Applies filtering functions to train, validation, and test splits to focus on specific classes ('Run', 'Pass', 'Punt', 'Kick-Off', 'Field_Goal'). Configures the maximum number of sequences to balance and limit the data effectively\n",
    "- Data Loader Customization: Implements a custom collate function to handle potential None entries in batches, ensuring robust data loading\n",
    "- Dataset Verification: Outputs class and sequence distribution for each dataset, ensuring correct setup and providing transparency about the data structure and contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, seq_length, max_sequences=None, transform=None):\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.transform = transform\n",
    "        self.sequences = []\n",
    "        self.label_distribution = defaultdict(int)\n",
    "\n",
    "        # Group frames by directory (label) and video ID\n",
    "        video_label_frames = defaultdict(list)\n",
    "        for path, label in self.image_folder_dataset.samples:\n",
    "            # Skip augmented frames\n",
    "            if path.split(os.sep)[-1].startswith('aug_'):\n",
    "                continue\n",
    "            path_elements = path.split(os.sep)\n",
    "            video_id = path_elements[-1].split('_frame_')[0]\n",
    "            label_dir = path_elements[-2]\n",
    "            full_id = f\"{label_dir}_{video_id}\"\n",
    "            video_label_frames[full_id].append((path, label))\n",
    "\n",
    "        # Process frames for each unique video-label combination\n",
    "        for frames in video_label_frames.values():\n",
    "            # Sort frames within the same video and label by frame number\n",
    "            frames.sort(key=lambda x: int(x[0].split('_frame_')[1].split('.')[0]))\n",
    "\n",
    "            # Create valid sequences\n",
    "            for i in range(len(frames) - seq_length + 1):\n",
    "                sequence = frames[i:i + seq_length]\n",
    "                if len(set(label for _, label in sequence)) == 1:  # Check if all labels in sequence are the same\n",
    "                    self.sequences.append(sequence)\n",
    "                    self.label_distribution[label] += 1\n",
    "\n",
    "        # Balance and limit number of sequences if max_sequences is set\n",
    "        if max_sequences:\n",
    "            balanced_sequences = []\n",
    "            min_count = min(self.label_distribution.values(), default=0)  # Avoid division by zero\n",
    "            limit_per_label = min(max_sequences // 2, min_count)\n",
    "\n",
    "            label_counters = defaultdict(int)\n",
    "            for seq in self.sequences:\n",
    "                label = seq[0][1]\n",
    "                if label_counters[label] < limit_per_label:\n",
    "                    balanced_sequences.append(seq)\n",
    "                    label_counters[label] += 1\n",
    "\n",
    "            self.sequences = balanced_sequences\n",
    "            self.label_distribution = label_counters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.sequences[idx]\n",
    "        images, labels, img_paths = [], [], []\n",
    "        for img_path, label in frames:\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img = img.convert('RGB')  # Ensure the image is in RGB mode\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                img_paths.append(img_path)\n",
    "            except IOError:\n",
    "                print(f\"Skipping frame {img_path} due to loading error.\")\n",
    "                continue\n",
    "\n",
    "        if len(images) != len(frames):  # Check if all images were loaded\n",
    "            return None  # Can be skipped in DataLoader logic\n",
    "\n",
    "        images = torch.stack(images)  # Stack images into a tensor\n",
    "        return images, torch.tensor(labels[0]), img_paths\n",
    "\n",
    "## Apply the filter to each dataset split\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    filter_classes(image_datasets[phase], ['Run', 'Pass', 'Punt', 'Kick-Off', 'Field_Goal'])\n",
    "\n",
    "# Print classes in each dataset to verify correct loading\n",
    "print(\"Classes in the training dataset:\", image_datasets['train'].classes)\n",
    "print(\"Classes in the validation dataset:\", image_datasets['val'].classes)\n",
    "print(\"Classes in the testing dataset:\", image_datasets['test'].classes)\n",
    "\n",
    "# Setup your datasets and dataloaders\n",
    "seq_length = 60  # Desired sequence length\n",
    "max_train_sequences = 10000  # Maximum allowable training sequences\n",
    "max_val_test_sequences = int(0.15 * max_train_sequences)  # Proportion for validation and test\n",
    "\n",
    "video_datasets = {\n",
    "    'train': VideoDataset(image_datasets['train'], seq_length, max_train_sequences, data_transforms['train']),\n",
    "    'val': VideoDataset(image_datasets['val'], seq_length, max_val_test_sequences, data_transforms['val']),\n",
    "    'test': VideoDataset(image_datasets['test'], seq_length, max_val_test_sequences, data_transforms['test'])\n",
    "}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]  # Entfernt None-EintrÃ¤ge\n",
    "    if not batch:\n",
    "        return torch.tensor([]), torch.tensor([]), []\n",
    "    images, labels, paths = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)\n",
    "    return images, labels, paths\n",
    "\n",
    "# Verwende diese Funktion in deinem DataLoader\n",
    "dataloaders = {\n",
    "    x: DataLoader(video_datasets[x], batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "\n",
    "# Print information about loaded data\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{phase.upper()} dataset: {len(video_datasets[phase])} sequences\")\n",
    "    for label in range(len(image_datasets[phase].classes)):  # Adjust to count all labels\n",
    "        print(f\"  Label {label} ({image_datasets[phase].classes[label]}): {video_datasets[phase].label_distribution[label]} sequences\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN-LSTM Network forMulticlas Classification:\n",
    "\n",
    "- Device Setup: Checks CUDA availability for GPU acceleration and sets the computation device accordingly\n",
    "- CNN-LSTM Architecture: Defines a hybrid neural network model combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) \n",
    "    - CNN: Utilizes a pre-trained ResNet-50 model, adapted for feature extraction\n",
    "    - LSTM: Processes temporal sequences with configurable hidden sizes and layer counts\n",
    "- Model Instantiation: Constructs the CNN-LSTM model with specific parameters and assigns it to the chosen computation device\n",
    "- Training Setup: Establishes the loss function, optimizer, and learning rate scheduler to guide the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MCI\\anaconda3\\envs\\football_analysis\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MCI\\anaconda3\\envs\\football_analysis\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and use it if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, cnn_model, hidden_size, num_classes=5, num_layers=1):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) # 2048 for ResNet\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, C, H, W = x.size()  # Extract the dimensions of the input\n",
    "\n",
    "        # Reshape input for CNN\n",
    "        \n",
    "        c_in = x.view(batch_size * seq_length, C, H, W) \n",
    "        c_out = self.cnn(c_in)  # Run through CNN for feature extraction\n",
    "        c_out = c_out.view(batch_size, seq_length, -1)  \n",
    "\n",
    "        # Run through LSTM for sequence processing\n",
    "        r_out, (h_n, c_n) = self.lstm(c_out)  # LSTM layer\n",
    "        out = self.fc(r_out[:, -1, :])  # Use last output of the LSTM for classification\n",
    "        return out\n",
    "\n",
    "# Load the pre-trained ResNet-50 model and modify output feature maps\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "cnn_model = nn.Sequential(*list(resnet.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "# Define the hidden size, input size, number of classes, and number of LSTM layers\n",
    "hidden_size = 256\n",
    "num_classes = 5 \n",
    "#num_classes = 1  # Binary classification with single output for BCEWithLogitsLoss\n",
    "num_layers = 2 # Example number of LSTM layers\n",
    "\n",
    "# Instantiate the combined CNN-LSTM model\n",
    "cnn_lstm_model = CNN_LSTM(cnn_model, hidden_size, num_classes, num_layers).to(device)\n",
    "\n",
    "# Define the criterion, optimizer, and learning rate scheduler\n",
    "#criterion = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss for binary classification (Sigmoid)\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification tasks (Softmax)\n",
    "\n",
    "optimizer_ft = optim.Adam(cnn_lstm_model.parameters(), lr=0.0001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Evaluation Process for the CNN-LSTM Model:\n",
    "\n",
    "- Initialization: Sets up the training process, initializing the best model weights and tracking training duration\n",
    "- Training Loop: Iterates over the specified number of epochs, dividing each epoch into training and validation phases\n",
    "- Batch Processing: Processes each batch, applying model predictions, calculating loss, and updating model parameters during the training phase\n",
    "- Performance Metrics: Calculates and displays running loss and accuracy for each batch, updating periodically to monitor performance\n",
    "- Validation and Metrics Storage: In the validation phase, calculates the confusion matrix and updates the best model if improved accuracy is detected\n",
    "- Finalization: Completes training, prints total duration, and loads the best model weights for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=3, batch_update_interval=100):\n",
    "    since = time.time()  # Track the start time for training duration\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  # Keep a copy of the best model weights\n",
    "    best_acc = 0.0  # Initialize the best accuracy\n",
    "\n",
    "    print(\"Training start\")  # Print training start message\n",
    "\n",
    "    for epoch in range(num_epochs):  # Loop over epochs\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:  # Each epoch has a training and validation phase\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0  # Initialize running los9\n",
    "        \n",
    "            running_corrects = 0  # Initialize running correct predictions\n",
    "            batch_count = 0  # Initialize batch count\n",
    "\n",
    "            all_labels = []  # Store all true labels\n",
    "            all_preds = []  # Store all predictions\n",
    "\n",
    "            data_iter = iter(dataloaders[phase])  # Create an iterator for the DataLoader\n",
    "            batch_total = len(dataloaders[phase])  # Total number of batches\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = next(data_iter)  # Get the next batch\n",
    "                    inputs, labels, _ = batch  # Get the first two elements only, ignore the rest\n",
    "                    \n",
    "                    # Check if the batch is empty and skip if true\n",
    "                    if inputs.size(0) == 0:\n",
    "                        print(\"Skipping empty batch.\")\n",
    "                        continue\n",
    "\n",
    "                except StopIteration:\n",
    "                    break  # Exit the loop if there are no more batches\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).long()\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Update running loss and correct predictions\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                batch_count += 1\n",
    "\n",
    "                # Collect labels and predictions for confusion matrix\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                # Print update every `batch_update_interval` batches\n",
    "                if batch_count % batch_update_interval == 0:\n",
    "                    print(f'Batch {batch_count}/{batch_total}: {phase} Loss: {running_loss / (batch_count * inputs.size(0)):.4f} Acc: {running_corrects.double() / (batch_count * inputs.size(0)):.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "            # Calculate epoch loss and accuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Print confusion matrix for validation phase\n",
    "            if phase == 'val':\n",
    "                cm = confusion_matrix(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n",
    "                print(f'Confusion Matrix for epoch {epoch}:\\n{cm}')\n",
    "\n",
    "            # Deep copy the best model weights and save the model if it has the best accuracy\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, 'best_model_play.pth')  # Save the best model weights\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Calculate total training time\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    #print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Instantiate the criterion for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate the model with live updates and confusion matrix printing\n",
    "model_ft = train_model(cnn_lstm_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of the CNN-LSTM Model on Test Data:\n",
    "\n",
    "- Model Loading: Loads the trained CNN-LSTM model from a specified file path and sets it to evaluation mode.\n",
    "- Loss and Accuracy Tracking: Initializes counters for running loss and correct predictions, and prepares to collect all labels and predictions for confusion matrix analysis\n",
    "- Batch Evaluation: Processes each batch from the test DataLoader, performing a forward pass to compute losses and predictions without gradient updates\n",
    "- Metrics Computation: Calculates final accuracy and loss for the test set and constructs the confusion matrix\n",
    "- Results Output: Prints the confusion matrix, final accuracy, and loss, providing a comprehensive evaluation of the model's performance on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_path, dataloaders, dataset_sizes, device, update_interval=50):\n",
    "    # Loading the saved model state and setting the model on the device\n",
    "    model = CNN_LSTM(cnn_model, hidden_size, num_classes=5, num_layers=2).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Not tracking gradients\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for inputs, labels, _ in dataloaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)  # Assuming `criterion` is defined\n",
    "\n",
    "            # Collecting results for the entire evaluation\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Update for every 50th batch\n",
    "            if (batch_count + 1) % update_interval == 0:\n",
    "                intermediate_acc = running_corrects.double() / ((batch_count + 1) * inputs.size(0))\n",
    "                intermediate_loss = running_loss / ((batch_count + 1) * inputs.size(0))\n",
    "                print(f'After {batch_count + 1} batches: Intermediate accuracy = {intermediate_acc:.4f}, Intermediate loss = {intermediate_loss:.4f}')\n",
    "            \n",
    "            batch_count += 1\n",
    "\n",
    "    # Calculating overall accuracy and loss\n",
    "    final_acc = running_corrects.double() / dataset_sizes['val']\n",
    "    final_loss = running_loss / dataset_sizes['val']\n",
    "    \n",
    "    # Creating confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f'Confusion matrix:\\n{cm}')\n",
    "    print(f'Final accuracy: {final_acc:.4f}')\n",
    "    print(f'Final loss: {final_loss:.4f}')\n",
    "\n",
    "# Configuring model path and device\n",
    "model_path = 'best_model_play.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model_path, dataloaders, dataset_sizes, device, update_interval=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
