{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import  transforms\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import defaultdict\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset sizes: {'train': 102566, 'val': 20193, 'test': 20513}\n",
      "Filtered classes: ['Run', 'Pass', 'Punt', 'Kick-Off', 'Field_Goal']\n",
      "\n",
      "TRAIN dataset:\n",
      "  Run: 37229 frames\n",
      "  Pass: 31412 frames\n",
      "  Punt: 13028 frames\n",
      "  Kick-Off: 11688 frames\n",
      "  Field_Goal: 9209 frames\n",
      "\n",
      "VAL dataset:\n",
      "  Run: 9603 frames\n",
      "  Pass: 4097 frames\n",
      "  Punt: 2234 frames\n",
      "  Kick-Off: 2373 frames\n",
      "  Field_Goal: 1886 frames\n",
      "\n",
      "TEST dataset:\n",
      "  Run: 6234 frames\n",
      "  Pass: 6153 frames\n",
      "  Punt: 3468 frames\n",
      "  Kick-Off: 2889 frames\n",
      "  Field_Goal: 1769 frames\n"
     ]
    }
   ],
   "source": [
    "# Define data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomRotation(degrees=10),\n",
    "        #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  \n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Path to your data directory\n",
    "data_dir = 'Sets'\n",
    "\n",
    "# Load data from directories, focusing on 'Play' and 'Time_Between' classes only\n",
    "def filter_classes(dataset, classes_to_include):\n",
    "    # Filter samples\n",
    "    filtered_samples = [(path, label) for path, label in dataset.samples if dataset.classes[label] in classes_to_include]\n",
    "    \n",
    "    # Reassign targets and samples\n",
    "    new_targets = []\n",
    "    new_samples = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes_to_include)}\n",
    "    \n",
    "    for path, label in filtered_samples:\n",
    "        class_name = dataset.classes[label]\n",
    "        if class_name in class_to_idx:\n",
    "            new_label = class_to_idx[class_name]\n",
    "            new_samples.append((path, new_label))\n",
    "            new_targets.append(new_label)\n",
    "    \n",
    "    dataset.samples = new_samples\n",
    "    dataset.targets = new_targets\n",
    "    dataset.classes = classes_to_include\n",
    "    dataset.class_to_idx = class_to_idx\n",
    "\n",
    "# Apply the filter to each dataset split\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    filter_classes(image_datasets[phase], ['Run', 'Pass', 'Punt', 'Kick-Off', 'Field_Goal'])\n",
    "\n",
    "# Check dataset sizes and class names\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Print dataset sizes and class names\n",
    "print(\"Filtered dataset sizes:\", dataset_sizes)\n",
    "print(\"Filtered classes:\", class_names)\n",
    "\n",
    "# Print number of frames for each class in each dataset\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{phase.upper()} dataset:\")\n",
    "    class_counts = {class_name: 0 for class_name in class_names}\n",
    "    for _, label in image_datasets[phase].samples:\n",
    "        class_name = class_names[label]\n",
    "        class_counts[class_name] += 1\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count} frames\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes in the training dataset: ['Run', 'Pass', 'Punt', 'Kick-Off', 'Field_Goal']\n",
      "Classes in the validation dataset: ['Run', 'Pass', 'Punt', 'Kick-Off', 'Field_Goal']\n",
      "Classes in the testing dataset: ['Run', 'Pass', 'Punt', 'Kick-Off', 'Field_Goal']\n",
      "\n",
      "TRAIN dataset: 20114 sequences\n",
      "  Label 0 (Run): 5000 sequences\n",
      "  Label 1 (Pass): 5000 sequences\n",
      "  Label 2 (Punt): 5000 sequences\n",
      "  Label 3 (Kick-Off): 3791 sequences\n",
      "  Label 4 (Field_Goal): 1323 sequences\n",
      "\n",
      "VAL dataset: 2906 sequences\n",
      "  Label 0 (Run): 750 sequences\n",
      "  Label 1 (Pass): 750 sequences\n",
      "  Label 2 (Punt): 538 sequences\n",
      "  Label 3 (Kick-Off): 677 sequences\n",
      "  Label 4 (Field_Goal): 191 sequences\n",
      "\n",
      "TEST dataset: 3074 sequences\n",
      "  Label 0 (Run): 750 sequences\n",
      "  Label 1 (Pass): 750 sequences\n",
      "  Label 2 (Punt): 750 sequences\n",
      "  Label 3 (Kick-Off): 750 sequences\n",
      "  Label 4 (Field_Goal): 74 sequences\n"
     ]
    }
   ],
   "source": [
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, seq_length, max_sequences=None, transform=None):\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.transform = transform\n",
    "        self.sequences = []\n",
    "        self.label_distribution = defaultdict(int)\n",
    "\n",
    "        # Group frames by directory (label) and video ID\n",
    "        video_label_frames = defaultdict(list)\n",
    "        for path, label in self.image_folder_dataset.samples:\n",
    "            # Skip augmented frames\n",
    "            if path.split(os.sep)[-1].startswith('aug_'):\n",
    "                continue\n",
    "            path_elements = path.split(os.sep)\n",
    "            video_id = path_elements[-1].split('_frame_')[0]\n",
    "            label_dir = path_elements[-2]\n",
    "            full_id = f\"{label_dir}_{video_id}\"\n",
    "            video_label_frames[full_id].append((path, label))\n",
    "\n",
    "        # Process frames for each unique video-label combination\n",
    "        for frames in video_label_frames.values():\n",
    "            # Sort frames within the same video and label by frame number\n",
    "            frames.sort(key=lambda x: int(x[0].split('_frame_')[1].split('.')[0]))\n",
    "\n",
    "            # Create valid sequences\n",
    "            for i in range(len(frames) - seq_length + 1):\n",
    "                sequence = frames[i:i + seq_length]\n",
    "                if len(set(label for _, label in sequence)) == 1:  # Check if all labels in sequence are the same\n",
    "                    self.sequences.append(sequence)\n",
    "                    self.label_distribution[label] += 1\n",
    "\n",
    "        # Balance and limit number of sequences if max_sequences is set\n",
    "        if max_sequences:\n",
    "            balanced_sequences = []\n",
    "            min_count = min(self.label_distribution.values(), default=0)  # Avoid division by zero\n",
    "            limit_per_label = min(max_sequences // 2, min_count)\n",
    "\n",
    "            label_counters = defaultdict(int)\n",
    "            for seq in self.sequences:\n",
    "                label = seq[0][1]\n",
    "                if label_counters[label] < limit_per_label:\n",
    "                    balanced_sequences.append(seq)\n",
    "                    label_counters[label] += 1\n",
    "\n",
    "            self.sequences = balanced_sequences\n",
    "            self.label_distribution = label_counters\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames = self.sequences[idx]\n",
    "        images, labels, img_paths = [], [], []\n",
    "        for img_path, label in frames:\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img = img.convert('RGB')  # Stelle sicher, dass das Bild im RGB-Modus ist\n",
    "                if self.transform:\n",
    "                    img = self.transform(img)\n",
    "                images.append(img)\n",
    "                labels.append(label)\n",
    "                img_paths.append(img_path)\n",
    "            except IOError:\n",
    "                print(f\"Skipping frame {img_path} due to loading error.\")\n",
    "                continue\n",
    "\n",
    "        if len(images) != len(frames):  # Prüft, ob alle Bilder geladen wurden\n",
    "            return None  # Kann in der Dataloader-Logik übersprungen werden\n",
    "\n",
    "        images = torch.stack(images)  # Stapelt die Bilder zu einem Tensor\n",
    "        return images, torch.tensor(labels[0]), img_paths\n",
    "\n",
    "## Apply the filter to each dataset split\n",
    "image_datasets = {\n",
    "    x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    filter_classes(image_datasets[phase], ['Run', 'Pass', 'Punt', 'Kick-Off', 'Field_Goal'])\n",
    "\n",
    "# Print classes in each dataset to verify correct loading\n",
    "print(\"Classes in the training dataset:\", image_datasets['train'].classes)\n",
    "print(\"Classes in the validation dataset:\", image_datasets['val'].classes)\n",
    "print(\"Classes in the testing dataset:\", image_datasets['test'].classes)\n",
    "\n",
    "# Setup your datasets and dataloaders\n",
    "seq_length = 60  # Desired sequence length\n",
    "max_train_sequences = 10000  # Maximum allowable training sequences\n",
    "max_val_test_sequences = int(0.15 * max_train_sequences)  # Proportion for validation and test\n",
    "\n",
    "video_datasets = {\n",
    "    'train': VideoDataset(image_datasets['train'], seq_length, max_train_sequences, data_transforms['train']),\n",
    "    'val': VideoDataset(image_datasets['val'], seq_length, max_val_test_sequences, data_transforms['val']),\n",
    "    'test': VideoDataset(image_datasets['test'], seq_length, max_val_test_sequences, data_transforms['test'])\n",
    "}\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    batch = [item for item in batch if item is not None]  # Entfernt None-Einträge\n",
    "    if not batch:\n",
    "        return torch.tensor([]), torch.tensor([]), []\n",
    "    images, labels, paths = zip(*batch)\n",
    "    images = torch.stack(images)\n",
    "    labels = torch.stack(labels)\n",
    "    return images, labels, paths\n",
    "\n",
    "# Verwende diese Funktion in deinem DataLoader\n",
    "dataloaders = {\n",
    "    x: DataLoader(video_datasets[x], batch_size=2, shuffle=True, collate_fn=custom_collate_fn)\n",
    "    for x in ['train', 'val', 'test']\n",
    "}\n",
    "\n",
    "# Print information about loaded data\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{phase.upper()} dataset: {len(video_datasets[phase])} sequences\")\n",
    "    for label in range(len(image_datasets[phase].classes)):  # Adjust to count all labels\n",
    "        print(f\"  Label {label} ({image_datasets[phase].classes[label]}): {video_datasets[phase].label_distribution[label]} sequences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MCI\\anaconda3\\envs\\football_analysis\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MCI\\anaconda3\\envs\\football_analysis\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Check if CUDA is available and use it if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, latent_dim=256, lstm_layers=2):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        # Laden des vortrainierten ResNet-50 Modells\n",
    "        resnet = models.resnet50(pretrained=True)\n",
    "        # Entfernen der letzten vollständig verbundenen Schicht (fc layer) des ResNet\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])\n",
    "        # LSTM Konfiguration\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=latent_dim, num_layers=lstm_layers, batch_first=True)\n",
    "        # Abschließende vollständig verbundene Schicht zur Klassifizierung\n",
    "        self.fc = nn.Linear(latent_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, C, H, W = x.size()\n",
    "        # Umformen für die CNN-Eingabe\n",
    "        c_in = x.view(batch_size * seq_length, C, H, W)\n",
    "        # Durchlaufen des CNN\n",
    "        c_out = self.cnn(c_in)\n",
    "        # Umformen für die LSTM-Eingabe\n",
    "        c_out = c_out.view(batch_size, seq_length, -1)\n",
    "        # Durchlaufen des LSTM\n",
    "        lstm_out, _ = self.lstm(c_out)\n",
    "        # Nutzung der letzten Ausgabe des LSTM für die Klassifikation\n",
    "        final_output = self.fc(lstm_out[:, -1, :])\n",
    "        return final_output\n",
    "\n",
    "# Überprüfung ob CUDA verfügbar ist\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Anzahl der Klassen definieren\n",
    "num_classes = 5\n",
    "\n",
    "# Instanziierung des CNN-LSTM Modells\n",
    "cnn_lstm_model = CNN_LSTM(num_classes).to(device)\n",
    "\n",
    "# Kriterium und Optimierer definieren\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer_ft = optim.Adam(cnn_lstm_model.parameters(), lr=0.0001)\n",
    "\n",
    "# Lernraten-Scheduler konfigurieren\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MCI\\anaconda3\\envs\\football_analysis\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\MCI\\anaconda3\\envs\\football_analysis\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and use it if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, cnn_model, hidden_size, num_classes=5, num_layers=1):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) # 2048 for ResNet\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, C, H, W = x.size()  # Extract the dimensions of the input\n",
    "\n",
    "        # Reshape input for CNN\n",
    "        \n",
    "        c_in = x.view(batch_size * seq_length, C, H, W) \n",
    "        c_out = self.cnn(c_in)  # Run through CNN for feature extraction\n",
    "        c_out = c_out.view(batch_size, seq_length, -1)  \n",
    "\n",
    "        # Run through LSTM for sequence processing\n",
    "        r_out, (h_n, c_n) = self.lstm(c_out)  # LSTM layer\n",
    "        out = self.fc(r_out[:, -1, :])  # Use last output of the LSTM for classification\n",
    "        return out\n",
    "\n",
    "# Load the pre-trained ResNet-50 model and modify output feature maps\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "cnn_model = nn.Sequential(*list(resnet.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "# Define the hidden size, input size, number of classes, and number of LSTM layers\n",
    "hidden_size = 256\n",
    "num_classes = 5 \n",
    "#num_classes = 1  # Binary classification with single output for BCEWithLogitsLoss\n",
    "num_layers = 2 # Example number of LSTM layers\n",
    "\n",
    "# Instantiate the combined CNN-LSTM model\n",
    "cnn_lstm_model = CNN_LSTM(cnn_model, hidden_size, num_classes, num_layers).to(device)\n",
    "\n",
    "# Define the criterion, optimizer, and learning rate scheduler\n",
    "#criterion = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss for binary classification (Sigmoid)\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification tasks (Softmax)\n",
    "\n",
    "optimizer_ft = optim.Adam(cnn_lstm_model.parameters(), lr=0.0001)\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=2, gamma=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training start\n",
      "Epoch 0/2\n",
      "----------\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 100/10057: train Loss: 1.3045 Acc: 0.4400\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 200/10057: train Loss: 1.2132 Acc: 0.4825\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 300/10057: train Loss: 1.1419 Acc: 0.5100\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 400/10057: train Loss: 1.0915 Acc: 0.5363\n",
      "Batch 500/10057: train Loss: 1.0440 Acc: 0.5620\n",
      "Batch 600/10057: train Loss: 0.9960 Acc: 0.5883\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 700/10057: train Loss: 0.9534 Acc: 0.6036\n",
      "Batch 800/10057: train Loss: 0.9297 Acc: 0.6106\n",
      "Batch 900/10057: train Loss: 0.9263 Acc: 0.6156\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1000/10057: train Loss: 0.8990 Acc: 0.6280\n",
      "Batch 1100/10057: train Loss: 0.8870 Acc: 0.6355\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1200/10057: train Loss: 0.8780 Acc: 0.6379\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1300/10057: train Loss: 0.8572 Acc: 0.6488\n",
      "Batch 1400/10057: train Loss: 0.8511 Acc: 0.6532\n",
      "Batch 1500/10057: train Loss: 0.8429 Acc: 0.6583\n",
      "Batch 1600/10057: train Loss: 0.8414 Acc: 0.6581\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1700/10057: train Loss: 0.8409 Acc: 0.6585\n",
      "Batch 1800/10057: train Loss: 0.8296 Acc: 0.6633\n",
      "Batch 1900/10057: train Loss: 0.8153 Acc: 0.6697\n",
      "Batch 2000/10057: train Loss: 0.8171 Acc: 0.6727\n",
      "Batch 2100/10057: train Loss: 0.8494 Acc: 0.6621\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 2200/10057: train Loss: 0.8601 Acc: 0.6580\n",
      "Batch 2300/10057: train Loss: 0.8632 Acc: 0.6554\n",
      "Batch 2400/10057: train Loss: 0.8590 Acc: 0.6573\n",
      "Batch 2500/10057: train Loss: 0.8551 Acc: 0.6598\n",
      "Batch 2600/10057: train Loss: 0.8499 Acc: 0.6608\n",
      "Batch 2700/10057: train Loss: 0.8484 Acc: 0.6626\n",
      "Batch 2800/10057: train Loss: 0.8419 Acc: 0.6654\n",
      "Batch 2900/10057: train Loss: 0.8329 Acc: 0.6698\n",
      "Batch 3000/10057: train Loss: 0.8233 Acc: 0.6730\n",
      "Batch 3100/10057: train Loss: 0.8093 Acc: 0.6795\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3200/10057: train Loss: 0.7962 Acc: 0.6855\n",
      "Batch 3300/10057: train Loss: 0.7881 Acc: 0.6889\n",
      "Batch 3400/10057: train Loss: 0.7761 Acc: 0.6947\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3500/10057: train Loss: 0.7642 Acc: 0.7001\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3600/10057: train Loss: 0.7555 Acc: 0.7039\n",
      "Batch 3700/10057: train Loss: 0.7442 Acc: 0.7089\n",
      "Batch 3800/10057: train Loss: 0.7328 Acc: 0.7141\n",
      "Batch 3900/10057: train Loss: 0.7233 Acc: 0.7181\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4000/10057: train Loss: 0.7121 Acc: 0.7230\n",
      "Batch 4100/10057: train Loss: 0.7031 Acc: 0.7274\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4200/10057: train Loss: 0.6970 Acc: 0.7301\n",
      "Batch 4300/10057: train Loss: 0.6951 Acc: 0.7303\n",
      "Batch 4400/10057: train Loss: 0.6899 Acc: 0.7327\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4500/10057: train Loss: 0.6950 Acc: 0.7319\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4600/10057: train Loss: 0.6900 Acc: 0.7346\n",
      "Batch 4700/10057: train Loss: 0.6815 Acc: 0.7383\n",
      "Batch 4800/10057: train Loss: 0.6724 Acc: 0.7421\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4900/10057: train Loss: 0.6669 Acc: 0.7443\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5000/10057: train Loss: 0.6610 Acc: 0.7465\n",
      "Batch 5100/10057: train Loss: 0.6553 Acc: 0.7490\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5200/10057: train Loss: 0.6488 Acc: 0.7517\n",
      "Batch 5300/10057: train Loss: 0.6409 Acc: 0.7548\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5400/10057: train Loss: 0.6348 Acc: 0.7573\n",
      "Batch 5500/10057: train Loss: 0.6277 Acc: 0.7601\n",
      "Batch 5600/10057: train Loss: 0.6255 Acc: 0.7616\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5700/10057: train Loss: 0.6197 Acc: 0.7637\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5800/10057: train Loss: 0.6116 Acc: 0.7669\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5900/10057: train Loss: 0.6038 Acc: 0.7699\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6000/10057: train Loss: 0.5969 Acc: 0.7724\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6100/10057: train Loss: 0.5898 Acc: 0.7748\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6200/10057: train Loss: 0.5830 Acc: 0.7774\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6300/10057: train Loss: 0.5752 Acc: 0.7805\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6400/10057: train Loss: 0.5840 Acc: 0.7771\n",
      "Batch 6500/10057: train Loss: 0.5853 Acc: 0.7759\n",
      "Batch 6600/10057: train Loss: 0.5871 Acc: 0.7733\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6700/10057: train Loss: 0.5860 Acc: 0.7727\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6800/10057: train Loss: 0.5851 Acc: 0.7715\n",
      "Batch 6900/10057: train Loss: 0.5850 Acc: 0.7712\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7000/10057: train Loss: 0.5839 Acc: 0.7716\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7100/10057: train Loss: 0.5803 Acc: 0.7730\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7200/10057: train Loss: 0.5759 Acc: 0.7747\n",
      "Batch 7300/10057: train Loss: 0.5757 Acc: 0.7747\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7400/10057: train Loss: 0.5724 Acc: 0.7763\n",
      "Batch 7500/10057: train Loss: 0.5679 Acc: 0.7784\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7600/10057: train Loss: 0.5652 Acc: 0.7798\n",
      "Batch 7700/10057: train Loss: 0.5602 Acc: 0.7821\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7800/10057: train Loss: 0.5587 Acc: 0.7829\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7900/10057: train Loss: 0.5548 Acc: 0.7846\n",
      "Batch 8000/10057: train Loss: 0.5504 Acc: 0.7863\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8100/10057: train Loss: 0.5463 Acc: 0.7879\n",
      "Batch 8200/10057: train Loss: 0.5417 Acc: 0.7898\n",
      "Batch 8300/10057: train Loss: 0.5417 Acc: 0.7901\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8400/10057: train Loss: 0.5375 Acc: 0.7918\n",
      "Batch 8500/10057: train Loss: 0.5340 Acc: 0.7932\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8600/10057: train Loss: 0.5294 Acc: 0.7952\n",
      "Batch 8700/10057: train Loss: 0.5255 Acc: 0.7967\n",
      "Batch 8800/10057: train Loss: 0.5215 Acc: 0.7985\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8900/10057: train Loss: 0.5172 Acc: 0.8002\n",
      "Batch 9000/10057: train Loss: 0.5127 Acc: 0.8021\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9100/10057: train Loss: 0.5098 Acc: 0.8030\n",
      "Batch 9200/10057: train Loss: 0.5067 Acc: 0.8043\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9300/10057: train Loss: 0.5026 Acc: 0.8059\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9400/10057: train Loss: 0.4991 Acc: 0.8073\n",
      "Batch 9500/10057: train Loss: 0.4950 Acc: 0.8091\n",
      "Batch 9600/10057: train Loss: 0.4919 Acc: 0.8103\n",
      "Batch 9700/10057: train Loss: 0.4878 Acc: 0.8120\n",
      "Batch 9800/10057: train Loss: 0.4834 Acc: 0.8138\n",
      "Batch 9900/10057: train Loss: 0.4796 Acc: 0.8152\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 10000/10057: train Loss: 0.4762 Acc: 0.8167\n",
      "train Loss: 0.0933 Acc: 0.1602\n",
      "Batch 100/1453: val Loss: 1.3065 Acc: 0.5350\n",
      "Batch 200/1453: val Loss: 1.4068 Acc: 0.4975\n",
      "Batch 300/1453: val Loss: 1.4437 Acc: 0.4883\n",
      "Batch 400/1453: val Loss: 1.4302 Acc: 0.4963\n",
      "Batch 500/1453: val Loss: 1.4164 Acc: 0.5020\n",
      "Batch 600/1453: val Loss: 1.4118 Acc: 0.4992\n",
      "Batch 700/1453: val Loss: 1.4096 Acc: 0.4993\n",
      "Batch 800/1453: val Loss: 1.4164 Acc: 0.4994\n",
      "Batch 900/1453: val Loss: 1.4134 Acc: 0.5061\n",
      "Batch 1000/1453: val Loss: 1.4033 Acc: 0.5095\n",
      "Batch 1100/1453: val Loss: 1.3997 Acc: 0.5109\n",
      "Batch 1200/1453: val Loss: 1.3750 Acc: 0.5221\n",
      "Batch 1300/1453: val Loss: 1.3772 Acc: 0.5215\n",
      "Batch 1400/1453: val Loss: 1.3721 Acc: 0.5250\n",
      "val Loss: 0.1969 Acc: 0.0758\n",
      "Confusion Matrix for epoch 0:\n",
      "[[706  44   0   0   0]\n",
      " [593 157   0   0   0]\n",
      " [ 12   0 102 345  79]\n",
      " [  0   0   0 565 112]\n",
      " [  0   0   0 191   0]]\n",
      "\n",
      "Epoch 1/2\n",
      "----------\n",
      "Batch 100/10057: train Loss: 0.3676 Acc: 0.8500\n",
      "Batch 200/10057: train Loss: 0.2263 Acc: 0.9175\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 300/10057: train Loss: 0.1839 Acc: 0.9333\n",
      "Batch 400/10057: train Loss: 0.1563 Acc: 0.9463\n",
      "Batch 500/10057: train Loss: 0.1483 Acc: 0.9500\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 600/10057: train Loss: 0.1318 Acc: 0.9567\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 700/10057: train Loss: 0.1242 Acc: 0.9600\n",
      "Batch 800/10057: train Loss: 0.1138 Acc: 0.9644\n",
      "Batch 900/10057: train Loss: 0.1027 Acc: 0.9683\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1000/10057: train Loss: 0.0981 Acc: 0.9680\n",
      "Batch 1100/10057: train Loss: 0.0983 Acc: 0.9686\n",
      "Batch 1200/10057: train Loss: 0.0954 Acc: 0.9700\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1300/10057: train Loss: 0.0920 Acc: 0.9704\n",
      "Batch 1400/10057: train Loss: 0.0992 Acc: 0.9682\n",
      "Batch 1500/10057: train Loss: 0.1004 Acc: 0.9680\n",
      "Batch 1600/10057: train Loss: 0.0963 Acc: 0.9697\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1700/10057: train Loss: 0.0976 Acc: 0.9691\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1800/10057: train Loss: 0.1041 Acc: 0.9656\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1900/10057: train Loss: 0.1030 Acc: 0.9661\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 2000/10057: train Loss: 0.1017 Acc: 0.9665\n",
      "Batch 2100/10057: train Loss: 0.0978 Acc: 0.9679\n",
      "Batch 2200/10057: train Loss: 0.0954 Acc: 0.9689\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 2300/10057: train Loss: 0.0941 Acc: 0.9691\n",
      "Batch 2400/10057: train Loss: 0.0906 Acc: 0.9704\n",
      "Batch 2500/10057: train Loss: 0.0882 Acc: 0.9712\n",
      "Batch 2600/10057: train Loss: 0.0913 Acc: 0.9704\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 2700/10057: train Loss: 0.1012 Acc: 0.9678\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 2800/10057: train Loss: 0.1035 Acc: 0.9668\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 2900/10057: train Loss: 0.1045 Acc: 0.9664\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3000/10057: train Loss: 0.1023 Acc: 0.9670\n",
      "Batch 3100/10057: train Loss: 0.1008 Acc: 0.9677\n",
      "Batch 3200/10057: train Loss: 0.0990 Acc: 0.9684\n",
      "Batch 3300/10057: train Loss: 0.0986 Acc: 0.9683\n",
      "Batch 3400/10057: train Loss: 0.0962 Acc: 0.9693\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3500/10057: train Loss: 0.0948 Acc: 0.9696\n",
      "Batch 3600/10057: train Loss: 0.0924 Acc: 0.9704\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3700/10057: train Loss: 0.0901 Acc: 0.9711\n",
      "Batch 3800/10057: train Loss: 0.0885 Acc: 0.9717\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3900/10057: train Loss: 0.0871 Acc: 0.9721\n",
      "Batch 4000/10057: train Loss: 0.0854 Acc: 0.9726\n",
      "Batch 4100/10057: train Loss: 0.0878 Acc: 0.9715\n",
      "Batch 4200/10057: train Loss: 0.0884 Acc: 0.9714\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4300/10057: train Loss: 0.0869 Acc: 0.9717\n",
      "Batch 4400/10057: train Loss: 0.0871 Acc: 0.9716\n",
      "Batch 4500/10057: train Loss: 0.0866 Acc: 0.9719\n",
      "Batch 4600/10057: train Loss: 0.0850 Acc: 0.9725\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4700/10057: train Loss: 0.0853 Acc: 0.9723\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4800/10057: train Loss: 0.0867 Acc: 0.9720\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4900/10057: train Loss: 0.0884 Acc: 0.9714\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5000/10057: train Loss: 0.0881 Acc: 0.9714\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5100/10057: train Loss: 0.0874 Acc: 0.9714\n",
      "Batch 5200/10057: train Loss: 0.0860 Acc: 0.9719\n",
      "Batch 5300/10057: train Loss: 0.0850 Acc: 0.9723\n",
      "Batch 5400/10057: train Loss: 0.0836 Acc: 0.9728\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5500/10057: train Loss: 0.0822 Acc: 0.9732\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5600/10057: train Loss: 0.0810 Acc: 0.9734\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5700/10057: train Loss: 0.0818 Acc: 0.9732\n",
      "Batch 5800/10057: train Loss: 0.0845 Acc: 0.9723\n",
      "Batch 5900/10057: train Loss: 0.0851 Acc: 0.9722\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6000/10057: train Loss: 0.0843 Acc: 0.9723\n",
      "Batch 6100/10057: train Loss: 0.0837 Acc: 0.9726\n",
      "Batch 6200/10057: train Loss: 0.0846 Acc: 0.9723\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6300/10057: train Loss: 0.0845 Acc: 0.9722\n",
      "Batch 6400/10057: train Loss: 0.0835 Acc: 0.9727\n",
      "Batch 6500/10057: train Loss: 0.0824 Acc: 0.9731\n",
      "Batch 6600/10057: train Loss: 0.0812 Acc: 0.9735\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6700/10057: train Loss: 0.0831 Acc: 0.9728\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6800/10057: train Loss: 0.0835 Acc: 0.9726\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6900/10057: train Loss: 0.0828 Acc: 0.9728\n",
      "Batch 7000/10057: train Loss: 0.0818 Acc: 0.9732\n",
      "Batch 7100/10057: train Loss: 0.0816 Acc: 0.9732\n",
      "Batch 7200/10057: train Loss: 0.0807 Acc: 0.9735\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7300/10057: train Loss: 0.0804 Acc: 0.9736\n",
      "Batch 7400/10057: train Loss: 0.0812 Acc: 0.9734\n",
      "Batch 7500/10057: train Loss: 0.0803 Acc: 0.9737\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7600/10057: train Loss: 0.0797 Acc: 0.9739\n",
      "Batch 7700/10057: train Loss: 0.0788 Acc: 0.9743\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7800/10057: train Loss: 0.0778 Acc: 0.9746\n",
      "Batch 7900/10057: train Loss: 0.0770 Acc: 0.9748\n",
      "Batch 8000/10057: train Loss: 0.0772 Acc: 0.9749\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8100/10057: train Loss: 0.0766 Acc: 0.9751\n",
      "Batch 8200/10057: train Loss: 0.0782 Acc: 0.9746\n",
      "Batch 8300/10057: train Loss: 0.0779 Acc: 0.9747\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8400/10057: train Loss: 0.0774 Acc: 0.9748\n",
      "Batch 8500/10057: train Loss: 0.0772 Acc: 0.9749\n",
      "Batch 8600/10057: train Loss: 0.0769 Acc: 0.9751\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8700/10057: train Loss: 0.0765 Acc: 0.9751\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8800/10057: train Loss: 0.0763 Acc: 0.9752\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8900/10057: train Loss: 0.0770 Acc: 0.9750\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9000/10057: train Loss: 0.0766 Acc: 0.9751\n",
      "Batch 9100/10057: train Loss: 0.0768 Acc: 0.9751\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9200/10057: train Loss: 0.0781 Acc: 0.9747\n",
      "Batch 9300/10057: train Loss: 0.0774 Acc: 0.9750\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9400/10057: train Loss: 0.0767 Acc: 0.9752\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9500/10057: train Loss: 0.0762 Acc: 0.9753\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9600/10057: train Loss: 0.0758 Acc: 0.9753\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9700/10057: train Loss: 0.0754 Acc: 0.9754\n",
      "Batch 9800/10057: train Loss: 0.0747 Acc: 0.9757\n",
      "Batch 9900/10057: train Loss: 0.0740 Acc: 0.9759\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 10000/10057: train Loss: 0.0733 Acc: 0.9761\n",
      "train Loss: 0.0143 Acc: 0.1914\n",
      "Batch 100/1453: val Loss: 3.1966 Acc: 0.5000\n",
      "Batch 200/1453: val Loss: 3.2555 Acc: 0.5000\n",
      "Batch 300/1453: val Loss: 3.1881 Acc: 0.5067\n",
      "Batch 400/1453: val Loss: 3.2239 Acc: 0.5012\n",
      "Batch 500/1453: val Loss: 3.2241 Acc: 0.5030\n",
      "Batch 600/1453: val Loss: 3.2584 Acc: 0.5008\n",
      "Batch 700/1453: val Loss: 3.2820 Acc: 0.5014\n",
      "Batch 800/1453: val Loss: 3.3242 Acc: 0.4956\n",
      "Batch 900/1453: val Loss: 3.3781 Acc: 0.4900\n",
      "Batch 1000/1453: val Loss: 3.4043 Acc: 0.4870\n",
      "Batch 1100/1453: val Loss: 3.4022 Acc: 0.4877\n",
      "Batch 1200/1453: val Loss: 3.3694 Acc: 0.4933\n",
      "Batch 1300/1453: val Loss: 3.3621 Acc: 0.4935\n",
      "Batch 1400/1453: val Loss: 3.3643 Acc: 0.4918\n",
      "val Loss: 0.4890 Acc: 0.0701\n",
      "Confusion Matrix for epoch 1:\n",
      "[[389 361   0   0   0]\n",
      " [482 268   0   0   0]\n",
      " [  0   0 321 192  25]\n",
      " [  0   0   0 438 239]\n",
      " [  0   0   0 191   0]]\n",
      "\n",
      "Epoch 2/2\n",
      "----------\n",
      "Batch 100/10057: train Loss: 0.0029 Acc: 1.0000\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 200/10057: train Loss: 0.0028 Acc: 0.9925\n",
      "Batch 300/10057: train Loss: 0.0031 Acc: 0.9950\n",
      "Batch 400/10057: train Loss: 0.0030 Acc: 0.9962\n",
      "Batch 500/10057: train Loss: 0.0029 Acc: 0.9970\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 600/10057: train Loss: 0.0028 Acc: 0.9967\n",
      "Batch 700/10057: train Loss: 0.0026 Acc: 0.9971\n",
      "Batch 800/10057: train Loss: 0.0026 Acc: 0.9975\n",
      "Batch 900/10057: train Loss: 0.0030 Acc: 0.9972\n",
      "Batch 1000/10057: train Loss: 0.0034 Acc: 0.9970\n",
      "Batch 1100/10057: train Loss: 0.0033 Acc: 0.9973\n",
      "Batch 1200/10057: train Loss: 0.0033 Acc: 0.9975\n",
      "Batch 1300/10057: train Loss: 0.0032 Acc: 0.9977\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1400/10057: train Loss: 0.0031 Acc: 0.9971\n",
      "Batch 1500/10057: train Loss: 0.0038 Acc: 0.9970\n",
      "Batch 1600/10057: train Loss: 0.0037 Acc: 0.9972\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1700/10057: train Loss: 0.0038 Acc: 0.9968\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 1800/10057: train Loss: 0.0038 Acc: 0.9964\n",
      "Batch 1900/10057: train Loss: 0.0037 Acc: 0.9966\n",
      "Batch 2000/10057: train Loss: 0.0036 Acc: 0.9968\n",
      "Batch 2100/10057: train Loss: 0.0036 Acc: 0.9969\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 2200/10057: train Loss: 0.0036 Acc: 0.9968\n",
      "Batch 2300/10057: train Loss: 0.0035 Acc: 0.9970\n",
      "Batch 2400/10057: train Loss: 0.0034 Acc: 0.9971\n",
      "Batch 2500/10057: train Loss: 0.0034 Acc: 0.9972\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 2600/10057: train Loss: 0.0033 Acc: 0.9971\n",
      "Batch 2700/10057: train Loss: 0.0033 Acc: 0.9972\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 2800/10057: train Loss: 0.0032 Acc: 0.9971\n",
      "Batch 2900/10057: train Loss: 0.0032 Acc: 0.9972\n",
      "Batch 3000/10057: train Loss: 0.0031 Acc: 0.9973\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3100/10057: train Loss: 0.0031 Acc: 0.9973\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3200/10057: train Loss: 0.0030 Acc: 0.9970\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3300/10057: train Loss: 0.0030 Acc: 0.9970\n",
      "Batch 3400/10057: train Loss: 0.0029 Acc: 0.9971\n",
      "Batch 3500/10057: train Loss: 0.0029 Acc: 0.9971\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3600/10057: train Loss: 0.0028 Acc: 0.9971\n",
      "Batch 3700/10057: train Loss: 0.0028 Acc: 0.9972\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 3800/10057: train Loss: 0.0028 Acc: 0.9971\n",
      "Batch 3900/10057: train Loss: 0.0027 Acc: 0.9972\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4000/10057: train Loss: 0.0027 Acc: 0.9971\n",
      "Batch 4100/10057: train Loss: 0.0027 Acc: 0.9972\n",
      "Batch 4200/10057: train Loss: 0.0027 Acc: 0.9973\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4300/10057: train Loss: 0.0027 Acc: 0.9972\n",
      "Batch 4400/10057: train Loss: 0.0026 Acc: 0.9973\n",
      "Batch 4500/10057: train Loss: 0.0026 Acc: 0.9973\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 4600/10057: train Loss: 0.0026 Acc: 0.9970\n",
      "Batch 4700/10057: train Loss: 0.0025 Acc: 0.9970\n",
      "Batch 4800/10057: train Loss: 0.0025 Acc: 0.9971\n",
      "Batch 4900/10057: train Loss: 0.0025 Acc: 0.9971\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5000/10057: train Loss: 0.0049 Acc: 1.9942\n",
      "Batch 5100/10057: train Loss: 0.0024 Acc: 0.9972\n",
      "Batch 5200/10057: train Loss: 0.0024 Acc: 0.9972\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5300/10057: train Loss: 0.0024 Acc: 0.9971\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5400/10057: train Loss: 0.0023 Acc: 0.9970\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5500/10057: train Loss: 0.0023 Acc: 0.9968\n",
      "Batch 5600/10057: train Loss: 0.0023 Acc: 0.9969\n",
      "Batch 5700/10057: train Loss: 0.0022 Acc: 0.9969\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 5800/10057: train Loss: 0.0022 Acc: 0.9969\n",
      "Batch 5900/10057: train Loss: 0.0022 Acc: 0.9969\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6000/10057: train Loss: 0.0022 Acc: 0.9969\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6100/10057: train Loss: 0.0021 Acc: 0.9968\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6200/10057: train Loss: 0.0042 Acc: 1.9935\n",
      "Batch 6300/10057: train Loss: 0.0021 Acc: 0.9968\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6400/10057: train Loss: 0.0021 Acc: 0.9967\n",
      "Batch 6500/10057: train Loss: 0.0020 Acc: 0.9968\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6600/10057: train Loss: 0.0020 Acc: 0.9967\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6700/10057: train Loss: 0.0020 Acc: 0.9966\n",
      "Batch 6800/10057: train Loss: 0.0020 Acc: 0.9967\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 6900/10057: train Loss: 0.0020 Acc: 0.9967\n",
      "Batch 7000/10057: train Loss: 0.0019 Acc: 0.9967\n",
      "Batch 7100/10057: train Loss: 0.0019 Acc: 0.9968\n",
      "Batch 7200/10057: train Loss: 0.0019 Acc: 0.9968\n",
      "Batch 7300/10057: train Loss: 0.0019 Acc: 0.9968\n",
      "Batch 7400/10057: train Loss: 0.0019 Acc: 0.9969\n",
      "Batch 7500/10057: train Loss: 0.0018 Acc: 0.9969\n",
      "Batch 7600/10057: train Loss: 0.0018 Acc: 0.9970\n",
      "Batch 7700/10057: train Loss: 0.0018 Acc: 0.9970\n",
      "Batch 7800/10057: train Loss: 0.0018 Acc: 0.9971\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 7900/10057: train Loss: 0.0018 Acc: 0.9970\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8000/10057: train Loss: 0.0017 Acc: 0.9970\n",
      "Batch 8100/10057: train Loss: 0.0017 Acc: 0.9970\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8200/10057: train Loss: 0.0017 Acc: 0.9970\n",
      "Batch 8300/10057: train Loss: 0.0017 Acc: 0.9970\n",
      "Batch 8400/10057: train Loss: 0.0017 Acc: 0.9971\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8500/10057: train Loss: 0.0017 Acc: 0.9971\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8600/10057: train Loss: 0.0016 Acc: 0.9970\n",
      "Batch 8700/10057: train Loss: 0.0016 Acc: 0.9971\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8800/10057: train Loss: 0.0016 Acc: 0.9970\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 8900/10057: train Loss: 0.0016 Acc: 0.9970\n",
      "Batch 9000/10057: train Loss: 0.0016 Acc: 0.9970\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9100/10057: train Loss: 0.0016 Acc: 0.9969\n",
      "Batch 9200/10057: train Loss: 0.0015 Acc: 0.9970\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9300/10057: train Loss: 0.0015 Acc: 0.9969\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9400/10057: train Loss: 0.0015 Acc: 0.9969\n",
      "Batch 9500/10057: train Loss: 0.0015 Acc: 0.9969\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9600/10057: train Loss: 0.0015 Acc: 0.9969\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9700/10057: train Loss: 0.0015 Acc: 0.9969\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9800/10057: train Loss: 0.0015 Acc: 0.9968\n",
      "Skipping frame Sets\\train\\Kick-Off\\GX010043_frame_1100.jpg due to loading error.\n",
      "Batch 9900/10057: train Loss: 0.0014 Acc: 0.9968\n",
      "Batch 10000/10057: train Loss: 0.0014 Acc: 0.9969\n",
      "train Loss: 0.0003 Acc: 0.1955\n",
      "Batch 100/1453: val Loss: 5.3879 Acc: 0.4200\n",
      "Batch 200/1453: val Loss: 5.0976 Acc: 0.4475\n",
      "Batch 300/1453: val Loss: 5.0351 Acc: 0.4550\n",
      "Batch 400/1453: val Loss: 5.0004 Acc: 0.4537\n",
      "Batch 500/1453: val Loss: 5.0005 Acc: 0.4500\n",
      "Batch 600/1453: val Loss: 4.9335 Acc: 0.4550\n",
      "Batch 700/1453: val Loss: 4.9152 Acc: 0.4621\n",
      "Batch 800/1453: val Loss: 4.9271 Acc: 0.4575\n",
      "Batch 900/1453: val Loss: 4.8841 Acc: 0.4611\n",
      "Batch 1000/1453: val Loss: 4.8772 Acc: 0.4600\n",
      "Batch 1100/1453: val Loss: 4.9043 Acc: 0.4545\n",
      "Batch 1200/1453: val Loss: 4.9201 Acc: 0.4538\n",
      "Batch 1300/1453: val Loss: 4.9452 Acc: 0.4496\n",
      "Batch 1400/1453: val Loss: 4.9635 Acc: 0.4493\n",
      "val Loss: 0.7156 Acc: 0.0645\n",
      "Confusion Matrix for epoch 2:\n",
      "[[394 356   0   0   0]\n",
      " [499 251   0   0   0]\n",
      " [  0   0 220 193 125]\n",
      " [  0   0   0 438 239]\n",
      " [  0   0   0 191   0]]\n",
      "\n",
      "Training complete in 1439m 16s\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=3, batch_update_interval=100):\n",
    "    since = time.time()  # Track the start time for training duration\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  # Keep a copy of the best model weights\n",
    "    best_acc = 0.0  # Initialize the best accuracy\n",
    "\n",
    "    print(\"Training start\")  # Print training start message\n",
    "\n",
    "    for epoch in range(num_epochs):  # Loop over epochs\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:  # Each epoch has a training and validation phase\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0  # Initialize running los9*/\n",
    "        \n",
    "            running_corrects = 0  # Initialize running correct predictions\n",
    "            batch_count = 0  # Initialize batch count\n",
    "\n",
    "            all_labels = []  # Store all true labels\n",
    "            all_preds = []  # Store all predictions\n",
    "\n",
    "            data_iter = iter(dataloaders[phase])  # Create an iterator for the DataLoader\n",
    "            batch_total = len(dataloaders[phase])  # Total number of batches\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = next(data_iter)  # Get the next batch\n",
    "                    inputs, labels, _ = batch  # Get the first two elements only, ignore the rest\n",
    "                    \n",
    "                    # Check if the batch is empty and skip if true\n",
    "                    if inputs.size(0) == 0:\n",
    "                        print(\"Skipping empty batch.\")\n",
    "                        continue\n",
    "\n",
    "                except StopIteration:\n",
    "                    break  # Exit the loop if there are no more batches\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).long()\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Update running loss and correct predictions\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                batch_count += 1\n",
    "\n",
    "                # Collect labels and predictions for confusion matrix\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                # Print update every `batch_update_interval` batches\n",
    "                if batch_count % batch_update_interval == 0:\n",
    "                    print(f'Batch {batch_count}/{batch_total}: {phase} Loss: {running_loss / (batch_count * inputs.size(0)):.4f} Acc: {running_corrects.double() / (batch_count * inputs.size(0)):.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "            # Calculate epoch loss and accuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Print confusion matrix for validation phase\n",
    "            if phase == 'val':\n",
    "                cm = confusion_matrix(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n",
    "                print(f'Confusion Matrix for epoch {epoch}:\\n{cm}')\n",
    "\n",
    "            # Deep copy the best model weights and save the model if it has the best accuracy\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, 'best_model_play.pth')  # Save the best model weights\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Calculate total training time\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    #print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Instantiate the criterion for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate the model with live updates and confusion matrix printing\n",
    "model_ft = train_model(cnn_lstm_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST 19.06.2024\n",
    "\n",
    "Gleiches Modell wie BSRQ\n",
    "seq_length = 30  # Desired sequence length\n",
    "max_train_sequences = 10000  # Maximum allowable training sequences\n",
    "\n",
    "54% \n",
    "[[704  46   0   0   0]\n",
    " [734  16   0   0   0]\n",
    " [  0   0 360 160  78]\n",
    " [  0   0   8 450 139]\n",
    " [  0   0   0 112 279]]\n",
    "\n",
    "TEST 22.06.2024\n",
    "Modell mit dem ursprünglich sehr gutes Multi-Klassen arbeiten funktioniert hat. Mit sehr kleinem Datensatz:\n",
    "[[600   0   0   0   0]\n",
    " [600   0   0   0   0]\n",
    " [  0   0 291   0 307]\n",
    " [  0   0   0 431 169]\n",
    " [  0   0   0 179  72]]\n",
    "\n",
    "\n",
    "3 Versuche noch geplant: \n",
    "1. Gleiches Modell mit weit mehr epochen \n",
    "2. Gleiches Modell mit höhrerer Datenzahl, also mehr Sequenzen \n",
    "3. Ürsprungl. Modell von 19.06. mit mehr daten und mehr epochen\n",
    "\n",
    "1. \n",
    "[[723  27   0   0   0]\n",
    " [734  16   0   0   0]\n",
    " [  0   0 196 210 192]\n",
    " [  0   0   0 350 387]\n",
    " [  0   0   0 160  91]]\n",
    " ~ 45% \n",
    "\n",
    "2. \n",
    "[[ 596  904    0    0    0]\n",
    " [1111  389    0    0    0]\n",
    " [   0    0  405    0  47]\n",
    " [   0    0    0  563  174]\n",
    " [   0    0    0  204   193]]\n",
    "\n",
    "2. Längere Sequenzen\n",
    "[[225   0   0   0   0]\n",
    " [225   0   0   0   0]\n",
    " [  0   0  34  67 77]\n",
    " [  0   0   0 225   0]\n",
    " [  0   0   0  67  124]]\n",
    "\n",
    "3. \n",
    "Test 30.06\n",
    "[[726  24   0   0   0]\n",
    " [734  16   0   0   0]\n",
    " [  0   0 274 264   0]\n",
    " [  0   0   1 656  20]\n",
    " [  0   0   0  18 173]]\n",
    "\n",
    "65%\n",
    "\n",
    "3. \n",
    "[[725  25   0   0   0]\n",
    " [728  22   0   0   0]\n",
    " [  0   0 327 211   0]\n",
    " [  0   0   0 650  27]\n",
    " [  0   0   0  17 174]]\n",
    "\n",
    "\n",
    "Last and Best: \n",
    "[[394 356   0   0   0]\n",
    " [499 251   0   0   0]\n",
    " [  0   0 220 193 125]\n",
    " [  0   0   0 438 239]\n",
    " [  0   0   0 191   0]]\n",
    "\n",
    "Test:\n",
    "[[706  44   0   0   0]\n",
    " [593 157   0   0   0]\n",
    " [ 12   0 102 345  79]\n",
    " [  0   0   0 565 112]\n",
    " [  0   0   0 191   0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 50 batches: Intermediate accuracy = 0.4900, Intermediate loss = 1.2569\n",
      "After 100 batches: Intermediate accuracy = 0.5200, Intermediate loss = 1.2109\n",
      "After 150 batches: Intermediate accuracy = 0.5300, Intermediate loss = 1.2638\n",
      "After 200 batches: Intermediate accuracy = 0.5175, Intermediate loss = 1.3105\n",
      "After 250 batches: Intermediate accuracy = 0.5140, Intermediate loss = 1.3675\n",
      "After 300 batches: Intermediate accuracy = 0.5067, Intermediate loss = 1.4068\n",
      "After 350 batches: Intermediate accuracy = 0.4971, Intermediate loss = 1.4264\n",
      "After 400 batches: Intermediate accuracy = 0.5062, Intermediate loss = 1.4174\n",
      "After 450 batches: Intermediate accuracy = 0.5089, Intermediate loss = 1.4137\n",
      "After 500 batches: Intermediate accuracy = 0.5060, Intermediate loss = 1.4206\n",
      "After 550 batches: Intermediate accuracy = 0.5073, Intermediate loss = 1.4197\n",
      "After 600 batches: Intermediate accuracy = 0.5117, Intermediate loss = 1.4036\n",
      "After 650 batches: Intermediate accuracy = 0.5115, Intermediate loss = 1.4168\n",
      "After 700 batches: Intermediate accuracy = 0.5157, Intermediate loss = 1.4145\n",
      "After 750 batches: Intermediate accuracy = 0.5160, Intermediate loss = 1.4191\n",
      "After 800 batches: Intermediate accuracy = 0.5162, Intermediate loss = 1.4163\n",
      "After 850 batches: Intermediate accuracy = 0.5218, Intermediate loss = 1.3942\n",
      "After 900 batches: Intermediate accuracy = 0.5194, Intermediate loss = 1.3959\n",
      "After 950 batches: Intermediate accuracy = 0.5200, Intermediate loss = 1.3944\n",
      "After 1000 batches: Intermediate accuracy = 0.5255, Intermediate loss = 1.3771\n",
      "After 1050 batches: Intermediate accuracy = 0.5229, Intermediate loss = 1.3776\n",
      "After 1100 batches: Intermediate accuracy = 0.5250, Intermediate loss = 1.3754\n",
      "After 1150 batches: Intermediate accuracy = 0.5222, Intermediate loss = 1.3759\n",
      "After 1200 batches: Intermediate accuracy = 0.5233, Intermediate loss = 1.3730\n",
      "After 1250 batches: Intermediate accuracy = 0.5224, Intermediate loss = 1.3779\n",
      "After 1300 batches: Intermediate accuracy = 0.5208, Intermediate loss = 1.3849\n",
      "After 1350 batches: Intermediate accuracy = 0.5230, Intermediate loss = 1.3824\n",
      "After 1400 batches: Intermediate accuracy = 0.5250, Intermediate loss = 1.3753\n",
      "After 1450 batches: Intermediate accuracy = 0.5269, Intermediate loss = 1.3685\n",
      "Confusion matrix:\n",
      "[[706  44   0   0   0]\n",
      " [593 157   0   0   0]\n",
      " [ 12   0 102 345  79]\n",
      " [  0   0   0 565 112]\n",
      " [  0   0   0 191   0]]\n",
      "Final accuracy: 0.0758\n",
      "Final loss: 0.1969\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model_path, dataloaders, dataset_sizes, device, update_interval=50):\n",
    "    # Loading the saved model state and setting the model on the device\n",
    "    model = CNN_LSTM(cnn_model, hidden_size, num_classes=5, num_layers=2).to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    # Not tracking gradients\n",
    "    with torch.no_grad():\n",
    "        batch_count = 0\n",
    "        for inputs, labels, _ in dataloaders['val']:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            loss = criterion(outputs, labels)  # Assuming `criterion` is defined\n",
    "\n",
    "            # Collecting results for the entire evaluation\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            # Update for every 50th batch\n",
    "            if (batch_count + 1) % update_interval == 0:\n",
    "                intermediate_acc = running_corrects.double() / ((batch_count + 1) * inputs.size(0))\n",
    "                intermediate_loss = running_loss / ((batch_count + 1) * inputs.size(0))\n",
    "                print(f'After {batch_count + 1} batches: Intermediate accuracy = {intermediate_acc:.4f}, Intermediate loss = {intermediate_loss:.4f}')\n",
    "            \n",
    "            batch_count += 1\n",
    "\n",
    "    # Calculating overall accuracy and loss\n",
    "    final_acc = running_corrects.double() / dataset_sizes['val']\n",
    "    final_loss = running_loss / dataset_sizes['val']\n",
    "    \n",
    "    # Creating confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(f'Confusion matrix:\\n{cm}')\n",
    "    print(f'Final accuracy: {final_acc:.4f}')\n",
    "    print(f'Final loss: {final_loss:.4f}')\n",
    "\n",
    "# Configuring model path and device\n",
    "model_path = 'best_model_play.pth'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the loss criterion\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_model(model_path, dataloaders, dataset_sizes, device, update_interval=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
