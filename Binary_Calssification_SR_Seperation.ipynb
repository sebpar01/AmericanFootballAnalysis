{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets\n",
    "from torchvision import  transforms\n",
    "from torchvision import models\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dataset sizes: {'train': 316795, 'val': 56903, 'test': 63966}\n",
      "Filtered classes: ['Play', 'Time_Between']\n",
      "\n",
      "TRAIN dataset:\n",
      "  Play: 128701 frames\n",
      "  Time_Between: 188094 frames\n",
      "\n",
      "VAL dataset:\n",
      "  Play: 23801 frames\n",
      "  Time_Between: 33102 frames\n",
      "\n",
      "TEST dataset:\n",
      "  Play: 26188 frames\n",
      "  Time_Between: 37778 frames\n"
     ]
    }
   ],
   "source": [
    "# Define data transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        #transforms.RandomHorizontalFlip(p=0.5),\n",
    "        #transforms.RandomRotation(degrees=10),\n",
    "        #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "  \n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Path to your data directory\n",
    "data_dir = 'Sets'\n",
    "\n",
    "# Load data from directories, focusing on 'Play' and 'Time_Between' classes only\n",
    "def filter_classes(dataset, classes_to_include):\n",
    "    # Filter samples\n",
    "    filtered_samples = [(path, label) for path, label in dataset.samples if dataset.classes[label] in classes_to_include]\n",
    "    \n",
    "    # Reassign targets and samples\n",
    "    new_targets = []\n",
    "    new_samples = []\n",
    "    class_to_idx = {cls: idx for idx, cls in enumerate(classes_to_include)}\n",
    "    \n",
    "    for path, label in filtered_samples:\n",
    "        class_name = dataset.classes[label]\n",
    "        if class_name in class_to_idx:\n",
    "            new_label = class_to_idx[class_name]\n",
    "            new_samples.append((path, new_label))\n",
    "            new_targets.append(new_label)\n",
    "    \n",
    "    dataset.samples = new_samples\n",
    "    dataset.targets = new_targets\n",
    "    dataset.classes = classes_to_include\n",
    "    dataset.class_to_idx = class_to_idx\n",
    "\n",
    "# Apply the filter to each dataset split\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    filter_classes(image_datasets[phase], ['Play', 'Time_Between'])\n",
    "\n",
    "# Check dataset sizes and class names\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "class_names = image_datasets['train'].classes\n",
    "\n",
    "# Print dataset sizes and class names\n",
    "print(\"Filtered dataset sizes:\", dataset_sizes)\n",
    "print(\"Filtered classes:\", class_names)\n",
    "\n",
    "# Print number of frames for each class in each dataset\n",
    "for phase in ['train', 'val', 'test']:\n",
    "    print(f\"\\n{phase.upper()} dataset:\")\n",
    "    class_counts = {class_name: 0 for class_name in class_names}\n",
    "    for _, label in image_datasets[phase].samples:\n",
    "        class_name = class_names[label]\n",
    "        class_counts[class_name] += 1\n",
    "    for class_name, count in class_counts.items():\n",
    "        print(f\"  {class_name}: {count} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced and reduced 99960 samples for dataset.\n",
      "Balanced and reduced 15000 samples for dataset.\n",
      "Balanced and reduced 15000 samples for dataset.\n",
      "Prepared 88566 valid sequences.\n",
      "Prepared 13090 valid sequences.\n",
      "Prepared 13191 valid sequences.\n",
      "TRAIN dataset: 88566 sequences\n",
      "VAL dataset: 13090 sequences\n",
      "TEST dataset: 13191 sequences\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def balance_and_reduce_sequences(dataset, seq_length, max_frames_per_class):\n",
    "    class_counts = defaultdict(list)\n",
    "    for idx, (path, label) in enumerate(dataset.samples):\n",
    "        class_name = dataset.classes[label]\n",
    "        class_counts[class_name].append((path, label))\n",
    "\n",
    "    max_sequences_per_class = max_frames_per_class // seq_length\n",
    "\n",
    "    balanced_samples = []\n",
    "    for class_name in class_counts:\n",
    "        samples = class_counts[class_name][:max_sequences_per_class * seq_length]\n",
    "        balanced_samples.extend(samples)\n",
    "\n",
    "    dataset.samples = balanced_samples\n",
    "    dataset.targets = [label for _, label in balanced_samples]\n",
    "    print(f\"Balanced and reduced {len(balanced_samples)} samples for dataset.\")\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, image_folder_dataset, seq_length, transform=None):\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.seq_length = seq_length\n",
    "        self.transform = transform\n",
    "        self.valid_sequences = self.prepare_sequences()\n",
    "        print(f\"Prepared {len(self.valid_sequences)} valid sequences.\")\n",
    "\n",
    "    def prepare_sequences(self):\n",
    "        grouped_frames = defaultdict(list)\n",
    "        for path, label in self.image_folder_dataset.samples:\n",
    "            video_name, frame_number = self.parse_frame_details(path)\n",
    "            grouped_frames[video_name].append((int(frame_number), path, label))\n",
    "\n",
    "        valid_sequences = []\n",
    "        for frames in grouped_frames.values():\n",
    "            frames.sort()  # Sortiere Frames nach ihrer Nummer\n",
    "            # Hier generieren wir überlappende Sequenzen\n",
    "            for i in range(len(frames) - self.seq_length + 1):  # Erlaubt Überlappung\n",
    "                # Extra check to ensure continuity\n",
    "                if frames[i + self.seq_length - 1][0] - frames[i][0] == self.seq_length - 1:\n",
    "                    sequence = [(frame[1], frame[2]) for frame in frames[i:i + self.seq_length]]\n",
    "                    valid_sequences.append(sequence)\n",
    "        return valid_sequences\n",
    "\n",
    "    def parse_frame_details(self, path):\n",
    "        basename = os.path.basename(path)\n",
    "        parts = basename.split('_')\n",
    "        video_name = '_'.join(parts[:-2])\n",
    "        frame_number = parts[-1].split('.')[0]\n",
    "        return video_name, frame_number\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.valid_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths, labels = zip(*self.valid_sequences[idx])\n",
    "        images = [self.load_transform_image(path) for path in frame_paths]\n",
    "        images = torch.stack(images)\n",
    "        label = torch.tensor(labels[0])\n",
    "        assert torch.all(torch.tensor(labels) == label), \"Mismatched labels in a sequence.\"\n",
    "        return images, label\n",
    "\n",
    "    def load_transform_image(self, path):\n",
    "        img = self.image_folder_dataset.loader(path)\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "seq_length = 60\n",
    "max_frames_train = 50000\n",
    "max_frames_val_test = 7500\n",
    "\n",
    "# Assume image_datasets and data_transforms are defined somewhere else\n",
    "balance_and_reduce_sequences(image_datasets['train'], seq_length, max_frames_train)\n",
    "balance_and_reduce_sequences(image_datasets['val'], seq_length, max_frames_val_test)\n",
    "balance_and_reduce_sequences(image_datasets['test'], seq_length, max_frames_val_test)\n",
    "\n",
    "video_datasets = {x: VideoDataset(image_datasets[x], seq_length, data_transforms[x]) for x in ['train', 'val', 'test']}\n",
    "dataloaders = {x: DataLoader(video_datasets[x], batch_size=1, shuffle=True) for x in ['train', 'val', 'test']}\n",
    "\n",
    "# Print number of sequences in each set\n",
    "for key, dataset in video_datasets.items():\n",
    "    print(f\"{key.upper()} dataset: {len(dataset)} sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Sequence - Label: Play\n",
      "  Sets\\train\\Play\\GH085451_frame_5725.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5726.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5727.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5728.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5729.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5730.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5731.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5732.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5733.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5734.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5735.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5736.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5737.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5738.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5739.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5740.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5741.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5742.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5743.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5744.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5745.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5746.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5747.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5748.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5749.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5750.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5751.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5752.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5753.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5754.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5755.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5756.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5757.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5758.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5759.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5760.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5761.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5762.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5763.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5764.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5765.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5766.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5767.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5768.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5769.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5770.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5771.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5772.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5773.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5774.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5775.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5776.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5777.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5778.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5779.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5780.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5781.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5782.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5783.jpg\n",
      "  Sets\\train\\Play\\GH085451_frame_5784.jpg\n",
      "\n",
      "\n",
      "Random Sequence - Label: Time_Between\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31466.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31467.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31468.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31469.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31470.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31471.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31472.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31473.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31474.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31475.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31476.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31477.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31478.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31479.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31480.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31481.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31482.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31483.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31484.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31485.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31486.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31487.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31488.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31489.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31490.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31491.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31492.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31493.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31494.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31495.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31496.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31497.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31498.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31499.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31500.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31501.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31502.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31503.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31504.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31505.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31506.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31507.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31508.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31509.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31510.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31511.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31512.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31513.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31514.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31515.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31516.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31517.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31518.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31519.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31520.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31521.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31522.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31523.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31524.jpg\n",
      "  Sets\\train\\Time_Between\\GH045451_frame_31525.jpg\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def print_random_sequences(dataset, num_sequences=2):\n",
    "    # select random sequence\n",
    "    for _ in range(num_sequences):\n",
    "        random_idx = random.randint(0, len(dataset) - 1)\n",
    "        images, label = dataset[random_idx]\n",
    "        print(f\"Random Sequence - Label: {dataset.image_folder_dataset.classes[label]}\")\n",
    "        for img_path, _ in dataset.valid_sequences[random_idx]:\n",
    "            print(f\"  {img_path}\")\n",
    "        print(\"\\n\")  \n",
    "\n",
    "print_random_sequences(video_datasets['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available and use it if possible\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, cnn_model, hidden_size, num_classes=2, num_layers=1):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.cnn = cnn_model\n",
    "        self.lstm = nn.LSTM(input_size=2048, hidden_size=hidden_size, num_layers=num_layers, batch_first=True) # 2048 for ResNet\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, C, H, W = x.size()  # Extract the dimensions of the input\n",
    "\n",
    "        # Reshape input for CNN\n",
    "        \n",
    "        c_in = x.view(batch_size * seq_length, C, H, W) \n",
    "        c_out = self.cnn(c_in)  # Run through CNN for feature extraction\n",
    "        c_out = c_out.view(batch_size, seq_length, -1)  \n",
    "\n",
    "        # Run through LSTM for sequence processing\n",
    "        r_out, (h_n, c_n) = self.lstm(c_out)  # LSTM layer\n",
    "        out = self.fc(r_out[:, -1, :])  # Use last output of the LSTM for classification\n",
    "        return out\n",
    "\n",
    "# Load the pre-trained ResNet-50 model and modify output feature maps\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "cnn_model = nn.Sequential(*list(resnet.children())[:-2], nn.AdaptiveAvgPool2d((1, 1)))\n",
    "\n",
    "# Define the hidden size, input size, number of classes, and number of LSTM layers\n",
    "hidden_size = 512\n",
    "num_classes = 2 \n",
    "#num_classes = 1  # Binary classification with single output for BCEWithLogitsLoss\n",
    "num_layers = 1 # Example number of LSTM layers\n",
    "\n",
    "# Instantiate the combined CNN-LSTM model\n",
    "cnn_lstm_model = CNN_LSTM(cnn_model, hidden_size, num_classes, num_layers).to(device)\n",
    "\n",
    "# Define the criterion, optimizer, and learning rate scheduler\n",
    "#criterion = nn.BCEWithLogitsLoss()  # BCEWithLogitsLoss for binary classification (Sigmoid)\n",
    "criterion = nn.CrossEntropyLoss()  # CrossEntropyLoss for classification tasks (Softmax)\n",
    "optimizer_ft = optim.Adam(cnn_lstm_model.parameters(), lr=0.0001)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=50, batch_update_interval=35):\n",
    "    since = time.time()  # Track the start time for training duration\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  # Keep a copy of the best model weights\n",
    "    best_acc = 0.0  # Initialize the best accuracy\n",
    "\n",
    "    print(\"Training start\")  # Print training start message\n",
    "\n",
    "    for epoch in range(num_epochs):  # Loop over epochs\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:  # Each epoch has a training and validation phase\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0  # Initialize running loss\n",
    "            running_corrects = 0  # Initialize running correct predictions\n",
    "            batch_count = 0  # Initialize batch count\n",
    "\n",
    "            all_labels = []  # Store all true labels\n",
    "            all_preds = []  # Store all predictions\n",
    "\n",
    "            data_iter = iter(dataloaders[phase])  # Create an iterator for the DataLoader\n",
    "            batch_total = len(dataloaders[phase])  # Total number of batches\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    batch = next(data_iter)  # Get the next batch\n",
    "                    inputs, labels, _ = batch  # Get the first two elements only, ignore the rest\n",
    "                except StopIteration:\n",
    "                    break  # Exit the loop if there are no more batches\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).long()  # Ensure labels are of type long for CrossEntropyLoss\n",
    "\n",
    "                optimizer.zero_grad()  # Zero the parameter gradients\n",
    "\n",
    "                # Forward pass\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Update running loss and correct predictions\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                batch_count += 1\n",
    "\n",
    "                # Collect labels and predictions for confusion matrix\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "                # Print update every `batch_update_interval` batches\n",
    "                if batch_count % batch_update_interval == 0:\n",
    "                    print(f'Batch {batch_count}/{batch_total}: {phase} Loss: {running_loss / (batch_count * inputs.size(0)):.4f} Acc: {running_corrects.double() / (batch_count * inputs.size(0)):.4f}')\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step()  # Step the learning rate scheduler\n",
    "\n",
    "            # Calculate epoch loss and accuracy\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            # Print confusion matrix for validation phase\n",
    "            if phase == 'val':\n",
    "                cm = confusion_matrix(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n",
    "                print(f'Confusion Matrix for epoch {epoch}:\\n{cm}')\n",
    "\n",
    "            # Deep copy the best model weights and save the model if it has the best accuracy\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                torch.save(best_model_wts, 'best_model.pth')  # Save the best model weights\n",
    "\n",
    "        print()\n",
    "\n",
    "    # Calculate total training time\n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}')\n",
    "\n",
    "    # Load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "# Instantiate the criterion for multi-class classification\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and evaluate the model with live updates and confusion matrix printing\n",
    "model_ft = train_model(cnn_lstm_model, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=35)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
